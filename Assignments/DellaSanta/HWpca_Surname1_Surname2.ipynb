{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6347f1d2-5cb0-4032-b637-4730e9b9383a",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: PCA Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905c56f-029b-4d04-9e68-739362350bd8",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "Fill the missing values in this text box and in the following code-cell.\n",
    "\n",
    "**Academic Year:** YYYY/YYYY\n",
    "\n",
    "### Team Members (Alphabetical Order):\n",
    "1. Surname1, Name1 (StudentID1);\n",
    "2. Surname2, Name2 (StudentID2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c068e99-0c8c-433f-be0b-f41534dc8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StudentID1 = 341967  # <-------- Fill in the missing value\n",
    "StudentID2 = 344860  # <-------- Fill in the missing value\n",
    "\n",
    "# StudentID1 = 345862  # <-------- Fill in the missing value\n",
    "# StudentID2 = 99999999  # <-------- Fill in the missing value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f22b-a92f-4898-b26d-e4ab6d49c3d9",
   "metadata": {},
   "source": [
    "## Starting Code-Cell \n",
    "### Attention: DO NOT CHANGE THE CODE INSIDE THE FOLLOWING CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84734de0-62ad-4c39-b4b4-886696d3a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############## DO NOT CHANGE THE CODE IN THIS CELL #################\n",
    "####################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "var_entertainment_feat_types = ['Interests', 'Movies', 'Music']\n",
    "var_personal_feat_types = ['Finance', 'Phobias']\n",
    "fixed_feat_types = ['Personality', 'Health']\n",
    "\n",
    "label_types = ['Demographic']\n",
    "\n",
    "\n",
    "variables_by_type = {\n",
    "    'Demographics': ['Age', 'Height', 'Weight', 'Number of siblings', \n",
    "                     'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type',\n",
    "                     'Home Type'],\n",
    "    'Finance': ['Finances', 'Shopping centres', 'Branded clothing', \n",
    "                'Entertainment spending', 'Spending on looks', \n",
    "                'Spending on gadgets', 'Spending on healthy eating'],\n",
    "    'Health': ['Smoking', 'Alcohol', 'Healthy eating'],\n",
    "    'Interests': ['History', 'Psychology', 'Politics', 'Mathematics', \n",
    "                  'Physics', 'Internet', 'PC', 'Economy Management', \n",
    "                  'Biology', 'Chemistry', 'Reading', 'Geography', \n",
    "                  'Foreign languages', 'Medicine', 'Law', 'Cars', \n",
    "                  'Art exhibitions', 'Religion', 'Countryside, outdoors', \n",
    "                  'Dancing', 'Musical instruments', 'Writing', 'Passive sport', \n",
    "                  'Active sport', 'Gardening', 'Celebrities', 'Shopping', \n",
    "                  'Science and technology', 'Theatre', 'Fun with friends', \n",
    "                  'Adrenaline sports', 'Pets'],\n",
    "    'Movies': ['Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', \n",
    "               'Sci-fi', 'War', 'Fantasy/Fairy tales', 'Animated', \n",
    "               'Documentary', 'Western', 'Action'],\n",
    "    'Music': ['Music', 'Slow songs or fast songs', 'Dance', 'Folk', \n",
    "              'Country', 'Classical music', 'Musical', 'Pop', 'Rock', \n",
    "              'Metal or Hardrock', 'Punk', 'Hiphop, Rap', 'Reggae, Ska', \n",
    "              'Swing, Jazz', 'Rock n roll', 'Alternative', 'Latino', \n",
    "              'Techno, Trance', 'Opera'],\n",
    "    'Personality': ['Daily events', 'Prioritising workload', \n",
    "                    'Writing notes', 'Workaholism', 'Thinking ahead', \n",
    "                    'Final judgement', 'Reliability', 'Keeping promises', \n",
    "                    'Loss of interest', 'Friends versus money', 'Funniness', \n",
    "                    'Fake', 'Criminal damage', 'Decision making', 'Elections', \n",
    "                    'Self-criticism', 'Judgment calls', 'Hypochondria', \n",
    "                    'Empathy', 'Eating to survive', 'Giving', \n",
    "                    'Compassion to animals', 'Borrowed stuff', \n",
    "                    'Loneliness', 'Cheating in school', 'Health', \n",
    "                    'Changing the past', 'God', 'Dreams', 'Charity', \n",
    "                    'Number of friends', 'Punctuality', 'Lying', 'Waiting', \n",
    "                    'New environment', 'Mood swings', 'Appearence and gestures', \n",
    "                    'Socializing', 'Achievements', 'Responding to a serious letter', \n",
    "                    'Children', 'Assertiveness', 'Getting angry', \n",
    "                    'Knowing the right people', 'Public speaking', \n",
    "                    'Unpopularity', 'Life struggles', 'Happiness in life', \n",
    "                    'Energy levels', 'Small - big dogs', 'Personality', \n",
    "                    'Finding lost valuables', 'Getting up', 'Interests or hobbies', \n",
    "                    \"Parents' advice\", 'Questionnaires or polls', 'Internet usage'],\n",
    "    'Phobias': ['Flying', 'Storm', 'Darkness', 'Heights', 'Spiders', 'Snakes', \n",
    "                'Rats', 'Ageing', 'Dangerous dogs', 'Fear of public speaking']\n",
    "}\n",
    "\n",
    "labels = variables_by_type['Demographics']\n",
    "\n",
    "try:\n",
    "    random_seed = min([StudentID1, StudentID2])\n",
    "except NameError:\n",
    "    random_seed = StudentID1\n",
    "\n",
    "def which_featgroups():\n",
    "    np.random.seed(random_seed)\n",
    "    these_entertainments = np.random.choice(var_entertainment_feat_types, 2, replace=False).tolist()\n",
    "    these_personal = np.random.choice(var_personal_feat_types, 1, replace=False).tolist()\n",
    "    these_types = fixed_feat_types + these_personal + these_entertainments\n",
    "    print('*** THESE ARE THE SELECTED TYPE OF VARIABLES:')\n",
    "    for k in these_types:\n",
    "        print(f'{k}')\n",
    "    print('*************************************')\n",
    "    return these_types\n",
    "\n",
    "def which_features(these_types):\n",
    "    np.random.seed(random_seed)\n",
    "    these_features = []\n",
    "    for type in these_types:\n",
    "        if type != 'Personality':\n",
    "            these_features += variables_by_type[type]\n",
    "        else:\n",
    "            these_features += np.random.choice(variables_by_type[type], \n",
    "                                               int(2 * (len(variables_by_type[type]) / 3)), \n",
    "                                               replace=False).tolist()\n",
    "    print('*** THESE ARE THE SELECTED FEATURES:')\n",
    "    for ft in these_features:\n",
    "        print(f'{ft}')\n",
    "    print('*************************************')\n",
    "    return these_features\n",
    "\n",
    "these_types = which_featgroups()\n",
    "these_features = which_features(these_types)\n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c9090-6065-4f25-bdc3-1b3cdad6c083",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "In the following cell, import all the modules you think are necessary for doing the homework, **among the ones listed and used during the laboratories of the course**.\n",
    "No extra modules are allowed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ad703d-e6d8-4239-8222-7aa91fbda3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy\n",
    "\n",
    "import sklearn.model_selection\n",
    "import sklearn.preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1143d0-e6a8-43dd-8bf8-0363842bac60",
   "metadata": {},
   "source": [
    "## Exercise 1. Preparing the Dataset\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. load the dataset \"_responses_hw.csv_\";\n",
    "2. create a working dataframe extracting from _responses_hw.csv_ the columns corresponding to the variables in _these_features_, and randomly selecting 2/3 of the rows. Let us call this dataframe _X_df_;\n",
    "3. analyze the obtained dataframe and performing cleansing/encoding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf103f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# # plt.figure()\n",
    "# # data_df[labels[1]].plot()\n",
    "\n",
    "# print(data_df[labels[1]].mode())\n",
    "# c = Counter(data_df[labels[1]])\n",
    "# print(c.total())\n",
    "# print(c)\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(c.keys(), c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74cf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "data_df = pd.read_csv(\"responses_hw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d477-033c-40fd-b31e-f7c03ddb2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3\n",
    "categorical = ['Smoking', 'Alcohol','Punctuality','Lying','Internet usage', 'Education', 'Gender', 'Hand', 'Only child', 'Home Town Type', 'Home Type']\n",
    "\n",
    "howToMap = {\n",
    "    'average':{\n",
    "        'Smoking':          {'never smoked':    0,  'tried smoking':                    1,  'former smoker':    2,  'current smoker':           3},\n",
    "        'Alcohol':          {'never':           0,  'social drinker':                   1,  'drink a lot':      2}, \n",
    "        'Punctuality':      {'early':           -1, 'on time':                          0,  'late':             1}, \n",
    "        'Lying':            {'never':           0,  'only to avoid hurting someone':    1,  'sometimes':        2,  'everytime it suits me':    3},\n",
    "        'Internet usage':   {'no time at all':  0,  'less than an hour a day':          1,  'few hours a day':  2,  'most of the day':          3},\n",
    "        'Education':        {\n",
    "            'currently a primary school pupil': 0, \n",
    "            'primary school': 1,\n",
    "            'secondary school': 2,\n",
    "            'college/bachelor degree': 3,\n",
    "            'masters degree': 4,\n",
    "            'doctorate degree': 5  \n",
    "        }\n",
    "    },\n",
    "    'special':{\n",
    "        'Gender':           {'female':              -1, 'male':             1}, \n",
    "        'Hand':             {'left':                -1, 'right':            1}, \n",
    "        'Only child':       {'no':                  -1, 'yes':              1}, \n",
    "        'Home Town Type':   {'village':             -1, 'city':             1}, \n",
    "        'Home Type' :       {'block of flats':      -1, 'house/bungalow':   1},\n",
    "    }\n",
    "}\n",
    "\n",
    "data_fixed_df = data_df.copy()\n",
    "\n",
    "for method, colsToMap in howToMap.items():\n",
    "    for colName, valueMapping in colsToMap.items():\n",
    "        colSeries = data_df[colName].copy()\n",
    "\n",
    "        # if method == 'average':\n",
    "        #     colSeries[colSeries.isna()] = colSeries.mean()    \n",
    "        # elif method == 'special':\n",
    "        #     colSeries[colSeries.isna()] = colSeries.mode()[0]\n",
    "        # else:\n",
    "        #     colSeries[colSeries.isna()] = 0\n",
    "\n",
    "        colSeries[colSeries.isna()] = colSeries.mode()[0]\n",
    "        \n",
    "        data_df[colName] = colSeries.copy()\n",
    "\n",
    "        for valueToMap, sobstitution in valueMapping.items():\n",
    "            colSeries[colSeries == valueToMap] = sobstitution\n",
    " \n",
    "        data_fixed_df[colName] = colSeries.copy()\n",
    "\n",
    "for col in data_df.columns:\n",
    "    colSeries = data_fixed_df[col].copy()\n",
    "\n",
    "    colSeries[colSeries.isna()] = colSeries.mode()[0]\n",
    "    \n",
    "    data_df[col] = colSeries.copy()\n",
    "    data_fixed_df[col] = colSeries\n",
    "\n",
    "for col in data_fixed_df:\n",
    "    data_fixed_df[col] = data_fixed_df[col].astype(float)\n",
    "    \n",
    "X_df:pd.DataFrame\n",
    "\n",
    "X_df, _ = sklearn.model_selection.train_test_split(data_fixed_df[these_features], test_size=0.33, random_state=random_seed)\n",
    "X_df = X_df.sort_index()\n",
    "\n",
    "display(X_df)\n",
    "display(X_df.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4058cb9-19fc-406c-8380-7e8053b43649",
   "metadata": {},
   "source": [
    "## Exercise 2. Analyzing the Variance and the PCs\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. create two new dataframes from _X_df_ applying a StandardScaler and a MinMaxscaler. Call these new dataframes as _Xstd_df_ and _Xmm_df_, respectively;\n",
    "2. compute the variance of all the features in _X_df_, _Xstd_df_, and _Xmm_df_ and **comment the results**;\n",
    "3. compute all the $n$ Principal Components (PCs) for each dataset _X_df_, _Xstd_df_, and _Xmm_df_. Then, visualize the curves of the cumulative explained variances and **comment the results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0f2237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "Xstd_df = pd.DataFrame(sklearn.preprocessing.StandardScaler().fit_transform(X_df), index=X_df.index, columns=X_df.columns)\n",
    "Xmm_df = pd.DataFrame(sklearn.preprocessing.MinMaxScaler((0,1)).fit_transform(X_df), index=X_df.index, columns=X_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fe494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# display(X_df.describe())\n",
    "# display(Xstd_df.describe())\n",
    "# display(Xmm_df.describe())\n",
    "\n",
    "def plot_variances(X:pd.DataFrame):    \n",
    "    plt.figure()\n",
    "    X.describe().loc['std', :].plot()\n",
    "    plt.xticks(ticks=range(0, len(X.columns), 10), labels=[f'x{i + 1}' for i in range(0, len(X.columns), 10)])\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Variance')\n",
    "    plt.title('Variance of the Features')\n",
    "    plt.show()\n",
    "\n",
    "plot_variances(X_df)\n",
    "plot_variances(Xstd_df)\n",
    "plot_variances(Xmm_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181358a6-6bb0-4f22-b395-34e3757fa16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "pca_x = PCA()\n",
    "pca_x_std = PCA()\n",
    "pca_x_mm = PCA()\n",
    "\n",
    "pca_x.fit(X_df)\n",
    "pca_x_std.fit(Xstd_df)\n",
    "pca_x_mm.fit(Xmm_df)\n",
    "\n",
    "Y_x = pca_x.transform(X_df)\n",
    "Y_x_std = pca_x.transform(Xstd_df)\n",
    "Y_x_mm = pca_x.transform(Xmm_df)\n",
    "\n",
    "def plot_explained_variance_ratio(pca:PCA, title:str):    \n",
    "    plt.figure()\n",
    "    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.title(title)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(ticks=np.arange(0, pca.n_features_in_, 10), \n",
    "            labels=[f'PC{i + 1}' for i in range(0, pca.n_features_in_, 10)])\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_explained_variance_ratio(pca_x, 'Responses')\n",
    "plot_explained_variance_ratio(pca_x_std, 'Responses with standardization')\n",
    "plot_explained_variance_ratio(pca_x_mm, 'Responses with minmax scaling')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1771a-09b2-41bc-bc4c-7e3f6f530021",
   "metadata": {},
   "source": [
    "## Exercise 3. Dimensionality Reduction and PC Interpretation\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two dataframes _Xstd_df_, and _Xmm_df_, compute a new PCA for performing a dimensionality reduction with respect to $m$ dimensions. The value of $m$ must be $$m = \\min\\{m', 5\\}\\,,$$ where $m'$ is the value required for obtaining $33\\%$ of the total variance.\n",
    "2. For both the cases, visualize all the PCs and give a name/interpretation to them. **Comment and motivate your interpretations**. If possible, **compare the differences among the results obtained** for _Xstd_df_ and _Xmm_df_.\n",
    "3. Perform the score graph for both the cases (_std_ and _mm_). If $m>3$, plot the score graph with respect to the first 3 PCs. All the **plots must show the names of the PCs on the axes** for better understanding the results.\n",
    "4. **Optional:** plot more score graphs, coloring the dots with respect to any label in the list _labels_ that you believe can be interesting. **Comment and analyze this optional plots**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6fe9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "a = np.cumsum(pca_x_std.explained_variance_ratio_)\n",
    "m_prime_std = a.shape[0] - a[a > 0.33].shape[0] + 1\n",
    "m_std = min(m_prime_std, 5)\n",
    "\n",
    "a = np.cumsum(pca_x_mm.explained_variance_ratio_)\n",
    "m_prime_mm = a.shape[0] - a[a > 0.33].shape[0] + 1\n",
    "m_mm = min(m_prime_mm, 5)\n",
    "\n",
    "pca_x_std_m = PCA(n_components=m_std)\n",
    "pca_x_mm_m = PCA(n_components=m_mm)\n",
    "\n",
    "Ystd_m_df = pca_x_std_m.fit_transform(Xstd_df)\n",
    "Ymm_m_df = pca_x_mm_m.fit_transform(Xmm_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db46e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a\n",
    "tab10 = cm.tab10.colors\n",
    "\n",
    "def get_index_categories(cmap_, cols, order, variables_by_type, these_types):\n",
    "    norm = plt.Normalize(0, len(these_types))\n",
    "    colors = []\n",
    "    for col in cols[order]:\n",
    "        for i, t in enumerate(these_types):\n",
    "            var_list = variables_by_type[t]\n",
    "\n",
    "            if col in var_list:\n",
    "                colors.append(cmap_(norm(i)))\n",
    "                break\n",
    "    return colors\n",
    "\n",
    "def create_custom_legend(cmap, cats): \n",
    "    norm = plt.Normalize(0, len(cats)) \n",
    "    legend_handles = [] \n",
    "    for i, t in enumerate(cats): \n",
    "        legend_handles.append(plt.Line2D([0], [0], color=cmap(norm(i)), lw=4, label=t))\n",
    "    \n",
    "    return legend_handles\n",
    "\n",
    "def plot_principal_components(pca:PCA, m:int, columns, title:str, pc_names:list[str], cmap_ = cm.gist_ncar):\n",
    "    pcmax = pca.components_[:m, :].max()*1.1\n",
    "    pcmin = pca.components_[:m, :].min()*1.1\n",
    "\n",
    "    eps = np.sqrt(1/pca.n_features_in_)\n",
    "\n",
    "    for i in range(m):\n",
    "        plt.figure(figsize=(10,15))\n",
    "\n",
    "        idx = np.argsort(pca.components_[i, :])\n",
    "\n",
    "        # plt.barh(np.arange(0, pca_x.n_features_in_, step) - bar_height_std/2, pca_x.components_[i, :][idx][::step], height=bar_height_mm, label='Normal')\n",
    "        plt.barh(np.arange(pca.n_features_in_), pca.components_[i, :][idx], color=get_index_categories(cmap_, columns, idx, variables_by_type, these_types))\n",
    "        plt.axvline(x=eps, color='red', linestyle='--', linewidth=2, label='Vertical Line at x=30')\n",
    "        plt.axvline(x=-eps, color='red', linestyle='--', linewidth=2, label='Vertical Line at x=30')\n",
    "\n",
    "        plt.yticks(ticks=np.arange(pca_x_std.n_features_in_), labels=columns[idx].tolist()) \n",
    "        plt.xlim((pcmin, pcmax))\n",
    "        plt.title(title + f', responses PC{i+1}: {pc_names[i]}, {100*pca.explained_variance_ratio_[i]:.2f}% of total variance')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        legend_handles = create_custom_legend(cmap_, these_types) \n",
    "        plt.legend(handles=legend_handles, title=\"Feature Types\")\n",
    "            \n",
    "        plt.show()\n",
    "\n",
    "def plot_principal_components_comparisons(pca_x_std_m:PCA, pca_x_mm_m:PCA, m_std:int, m_mm:int, columns:list[str]):\n",
    "    bar_space = 0.4\n",
    "    bar_height = 0.2\n",
    "\n",
    "    for i in range(min(m_std, m_mm)):\n",
    "        plt.figure(figsize=(10,15)) \n",
    "        idx = np.argsort(pca_x_std_m.components_[i, :])\n",
    "        dir = np.sign(pca_x_std_m.components_[i, idx[0]]) * np.sign(pca_x_mm_m.components_[i, idx[0]])\n",
    "\n",
    "        plt.barh(np.arange(pca_x_std_m.n_features_in_) + bar_space/2, pca_x_std_m.components_[i, idx], height=bar_height, label='Standardized')\n",
    "        plt.barh(np.arange(pca_x_mm_m.n_features_in_)  - bar_space/2, dir * pca_x_mm_m.components_[i, idx], height=bar_height, label='Min-Max Scaled')\n",
    "        \n",
    "        plt.yticks(ticks=np.arange(pca_x_std_m.n_features_in_), labels=columns[idx].tolist()) \n",
    "        plt.title(f'Responses PC{i+1}')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "pc_names_std = ['Expressive people', 'Women', 'Young Mens', 'Respectfull/Rebellious Peoples', 'Milaneins/Scientist']\n",
    "pc_names_mm = ['Extroverts', 'Women', 'Young Mens', 'Path of Exile', 'Milaneins/Policoresi']\n",
    "# plot_principal_components(pca_x_std_m, m_std, X_df.columns, \"Standardized\", pc_names_std)\n",
    "# plot_principal_components(pca_x_mm_m, m_mm, X_df.columns, \"MinMax\", pc_names_mm)\n",
    "\n",
    "# 2.b\n",
    "# plot_principal_components_comparisons(pca_x_std_m, pca_x_mm_m, m_std, m_mm, X_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edb6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3\n",
    "def score_graph2D(Y, title, ax_names, colors, all_same_color=False, mappable=None):\n",
    "    scoregraph = plt.figure()\n",
    "    ax = scoregraph.add_subplot()\n",
    "    if all_same_color:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], color=colors)\n",
    "    else:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], c=colors)\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel(ax_names[0])\n",
    "    ax.set_ylabel(ax_names[1])\n",
    "\n",
    "    if mappable is not None:\n",
    "        plt.colorbar(mappable=mappable, ax=ax)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    return ax\n",
    "\n",
    "def score_graph3D(Y, title, ax_names, colors, all_same_color=False, mappable=None):\n",
    "    scoregraph = plt.figure()\n",
    "    ax = scoregraph.add_subplot(111, projection='3d')    \n",
    "    if all_same_color:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], color=colors)\n",
    "    else:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], c=colors)\n",
    "    plt.title(title)\n",
    "    ax.set_xlabel(ax_names[0])\n",
    "    ax.set_ylabel(ax_names[1])\n",
    "    ax.set_zlabel(ax_names[2])\n",
    "    \n",
    "    if mappable is not None:\n",
    "        plt.colorbar(mappable=mappable, ax=ax)\n",
    "    \n",
    "    plt.grid()\n",
    "    return ax\n",
    "    \n",
    "score_graph2D(Ystd_m_df, \"STANDARDIZED - SCORE GRAPH\", pc_names_std,   tab10[0],   True); plt.show()\n",
    "score_graph2D(Ymm_m_df,  \"MINMAX - SCORE GRAPH\",       pc_names_mm,    tab10[1],   True); plt.show()\n",
    "score_graph3D(Ystd_m_df, \"STANDARDIZED - SCORE GRAPH\", pc_names_std,   tab10[0],   True); plt.show()\n",
    "score_graph3D(Ymm_m_df,  \"MINMAX - SCORE GRAPH\",       pc_names_mm,    tab10[1],   True); plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "858b4e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mapping(values:list):\n",
    "#     unique_values = set(values) # Create a mapping from value to sequence number \n",
    "#     value_to_sequence = {value: i for i, value in enumerate(unique_values)} # Map the original values to the sequence \n",
    "#     mapped_values = [value_to_sequence[value] for value in values]\n",
    "\n",
    "#     return mapped_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48406cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4\n",
    "cat = labels[4]\n",
    "color_values = data_fixed_df.loc[X_df.index, cat]\n",
    "norm = plt.Normalize(vmin=np.min(color_values), vmax=np.max(color_values)) \n",
    "mappable = cm.ScalarMappable(norm=norm, cmap=cm.viridis)\n",
    "\n",
    "score_graph2D(Ystd_m_df, f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", pc_names_std, color_values,  False, mappable); plt.show()\n",
    "score_graph2D(Ymm_m_df,  f\"MINMAX - SCORE GRAPH, colored on {cat}\",       pc_names_mm,  color_values,  False, mappable); plt.show()\n",
    "score_graph3D(Ystd_m_df, f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", pc_names_std, color_values,  False, mappable); plt.show()\n",
    "score_graph3D(Ymm_m_df,  f\"MINMAX - SCORE GRAPH, colored on {cat}\",       pc_names_mm,  color_values,  False, mappable); plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8f0b8-37c5-403a-93ad-8abefd96e3b2",
   "metadata": {},
   "source": [
    "## Exercise 4. $k$-Means\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), run the $k$-Means for clustering the data. In particular, **use the silohuette score for identify the best value for $k\\in\\{3, \\ldots, 10\\}$**.\n",
    "2. Plot the score graphs of exercise 3.3, adding the centroids of the cluster.\n",
    "3. Observing the centroids coordinates in the PC space, **give a name/interpretation to them**, exploiting the names you assigned to the PCs. **Comment and motivate your interpretations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7be2-1756-4e44-9bcd-56a0fa7c78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def run_kmeans(Y)-> KMeans:\n",
    "    km_list = []\n",
    "    silcoeff_list = []\n",
    "    k_list = list(range(3, 11))\n",
    "\n",
    "    for i in range(len(k_list)):\n",
    "        print(f'****************** START k-MEANS WITH k={k_list[i]} ******************')\n",
    "        print('Computing...')\n",
    "        km_list.append(KMeans(n_clusters=k_list[i], random_state=random_seed))\n",
    "        km = km_list[i]\n",
    "        km.fit(Y)\n",
    "        silcoeff_list.append(silhouette_score(Y, labels=km.fit_predict(Y)))\n",
    "        print(f'****************** END k-MEANS WITH k={k_list[i]} ******************')\n",
    "        print('')\n",
    "\n",
    "    i_best = np.argmax(silcoeff_list)\n",
    "    k = k_list[i_best]\n",
    "    km = km_list[i_best]\n",
    "\n",
    "    print('****************** RESULTS OF THE SEARCH... ******************')\n",
    "    print(f'BEST SILHOUETTE SCORE: {silcoeff_list[i_best]} --> k = {k}')\n",
    "    print('**************************************************************')\n",
    "\n",
    "    print(silcoeff_list)\n",
    "\n",
    "    return km\n",
    "\n",
    "km_std_best = run_kmeans(Ystd_m_df)\n",
    "km_mm_best = run_kmeans(Ymm_m_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: change palette with lighter colors\n",
    "# 2\n",
    "cat = 'Gender'\n",
    "color_values = data_fixed_df.loc[X_df.index, cat]\n",
    "norm = plt.Normalize(vmin=np.min(color_values), vmax=np.max(color_values)) \n",
    "mappable = cm.ScalarMappable(norm=norm, cmap=cm.viridis)\n",
    "\n",
    "cluster_assignment_std = km_std_best.predict(Ystd_m_df)\n",
    "cluster_assignment_mm = km_mm_best.predict(Ymm_m_df)\n",
    "\n",
    "ax = score_graph2D(Ystd_m_df, f'STANDARDIZED - SCORE GRAPH, colored based on {cat}', pc_names_std, color_values, False, mappable)\n",
    "ax.scatter(km_std_best.cluster_centers_[:, 0], km_std_best.cluster_centers_[:, 1], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "ax = score_graph2D(Ystd_m_df, f'STANDARDIZED - SCORE GRAPH, colored based on cluster assignment', pc_names_std, cluster_assignment_std, False)\n",
    "ax.scatter(km_std_best.cluster_centers_[:, 0], km_std_best.cluster_centers_[:, 1], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "ax = score_graph2D(Ymm_m_df, f'MINMAX - SCORE GRAPH, colored based on {cat}', pc_names_mm, color_values, False, mappable)\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, 0], km_mm_best.cluster_centers_[:, 1], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "ax = score_graph2D(Ymm_m_df, f'MINMAX - SCORE GRAPH, colored based on cluster assignment', pc_names_mm, cluster_assignment_mm, False)\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, 0], km_mm_best.cluster_centers_[:, 1], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "ax = score_graph3D(Ystd_m_df, f'STANDARDIZED - SCORE GRAPH, colored based on {cat}', pc_names_std, color_values, False, mappable)\n",
    "ax.scatter(km_std_best.cluster_centers_[:, 0], km_std_best.cluster_centers_[:, 1], km_std_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "ax = score_graph3D(Ystd_m_df, f'STANDARDIZED - SCORE GRAPH, colored based on cluster assignment', pc_names_std, cluster_assignment_std, False)\n",
    "ax.scatter(km_std_best.cluster_centers_[:, 0], km_std_best.cluster_centers_[:, 1], km_std_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "ax = score_graph3D(Ymm_m_df, f'MINMAX - SCORE GRAPH, colored based on {cat}', pc_names_mm, color_values, False, mappable)\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, 0], km_mm_best.cluster_centers_[:, 1], km_mm_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "ax = score_graph3D(Ymm_m_df, f'MINMAX - SCORE GRAPH, colored based on cluster assignment', pc_names_mm, cluster_assignment_mm, False)\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, 0], km_mm_best.cluster_centers_[:, 1], km_mm_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb169142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "maxs_std_m = Ystd_m_df.max(axis=0) \n",
    "mins_std_m = Ystd_m_df.min(axis=0) \n",
    "\n",
    "maxs_mm_m = Ymm_m_df.max(axis=0) \n",
    "mins_mm_m = Ymm_m_df.min(axis=0) \n",
    "\n",
    "k_std = km_std_best.cluster_centers_.shape[0]\n",
    "k_mm = km_std_best.cluster_centers_.shape[0]\n",
    "\n",
    "fig_std, ax_std = plt.subplots(1, k_std, figsize=(15, 5))\n",
    "for ii in range(k_std):\n",
    "    ax_std[ii].bar(np.arange(km_mm_best.cluster_centers_.shape[1]), maxs_mm_m, color='blue', alpha=0.15)\n",
    "    ax_std[ii].bar(np.arange(km_mm_best.cluster_centers_.shape[1]), mins_mm_m, color='blue', alpha=0.15)\n",
    "    ax_std[ii].bar(np.arange(km_mm_best.cluster_centers_.shape[1]), km_mm_best.cluster_centers_[ii, :])\n",
    "    ax_std[ii].set_xticks(ticks=np.arange(km_mm_best.cluster_centers_.shape[1]))\n",
    "    ax_std[ii].set_xticklabels(labels=pc_names_mm, rotation=45)\n",
    "    ax_std[ii].grid(visible=True, which='both')\n",
    "    ax_std[ii].set_title(f'MinMax - CENTROID {ii+1}')\n",
    "\n",
    "\n",
    "fig_mm, ax_mm = plt.subplots(1, k_mm, figsize=(15, 5))\n",
    "for ii in range(k_mm):\n",
    "    ax_mm[ii].bar(np.arange(km_std_best.cluster_centers_.shape[1]), maxs_std_m, color='blue', alpha=0.15)\n",
    "    ax_mm[ii].bar(np.arange(km_std_best.cluster_centers_.shape[1]), mins_std_m, color='blue', alpha=0.15)\n",
    "    ax_mm[ii].bar(np.arange(km_std_best.cluster_centers_.shape[1]), km_std_best.cluster_centers_[ii, :])\n",
    "    ax_mm[ii].set_xticks(ticks=np.arange(km_std_best.cluster_centers_.shape[1]))\n",
    "    ax_mm[ii].set_xticklabels(labels=pc_names_std, rotation=45)\n",
    "    ax_mm[ii].grid(visible=True, which='both')\n",
    "    ax_mm[ii].set_title(f'Standard - CENTROID {ii+1}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c165c3e8-1dbb-46c0-b09e-d7704464c324",
   "metadata": {},
   "source": [
    "## Exercise 5. Cluster Evaluations\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), perform an **external evaluation** of the clustering obtained at exercise 4.1 with respect to one or more labels in the list _labels_. **Comment the results, comparing the evaluation with the interpretation you gave at exercise 4.3**. \n",
    "2. For each one of the two datasets (_std_ and _mm_), perform an **internal evaluation** of each cluster, with respect to the silohuette score. **Comment the results**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def external_evaluation(cluster_assignment, label_series):\n",
    "    distribution = pd.crosstab(cluster_assignment, label_series)\n",
    "\n",
    "    distribution.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "\n",
    "external_evaluation(cluster_assignment_std, data_fixed_df.loc[Xstd_df.index, labels[4]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babe2f4-f800-4f51-9c97-6059d4ca12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def internal_evaluation(kmean:KMeans, Y, cluster_assignment):\n",
    "    n_clusters = kmean.cluster_centers_.shape[0]\n",
    "\n",
    "    sil_sample_series = silhouette_samples(Y, labels=cluster_assignment)\n",
    "    y_lower = 10\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = np.sort(sil_sample_series[cluster_assignment == i])\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        plt.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        y_lower = y_upper +10\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "internal_evaluation(km_std_best, Ystd_m_df, cluster_assignment_std)\n",
    "# internal_evaluation(km_mm_best, Ymm_m_df, cluster_assignment_mm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
