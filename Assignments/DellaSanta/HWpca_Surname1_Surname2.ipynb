{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6347f1d2-5cb0-4032-b637-4730e9b9383a",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: PCA Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905c56f-029b-4d04-9e68-739362350bd8",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "Fill the missing values in this text box and in the following code-cell.\n",
    "\n",
    "**Academic Year:** 2024/2025\n",
    "\n",
    "### Team Members (Alphabetical Order):\n",
    "1. Licalzi, Nunzio (344860);\n",
    "2. Vercellone, Romeo (341967)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c068e99-0c8c-433f-be0b-f41534dc8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StudentID1 = 341967  # <-------- Fill in the missing value\n",
    "StudentID2 = 344860  # <-------- Fill in the missing value\n",
    "\n",
    "# StudentID1 = 345862  # <-------- Fill in the missing value\n",
    "# StudentID2 = 99999999  # <-------- Fill in the missing value\n",
    "\n",
    "# StudentID1 = 343438  # <-------- Fill in the missing value\n",
    "# StudentID2 = 99999999  # <-------- Fill in the missing value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f22b-a92f-4898-b26d-e4ab6d49c3d9",
   "metadata": {},
   "source": [
    "## Starting Code-Cell \n",
    "### Attention: DO NOT CHANGE THE CODE INSIDE THE FOLLOWING CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84734de0-62ad-4c39-b4b4-886696d3a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############## DO NOT CHANGE THE CODE IN THIS CELL #################\n",
    "####################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "var_entertainment_feat_types = ['Interests', 'Movies', 'Music']\n",
    "var_personal_feat_types = ['Finance', 'Phobias']\n",
    "fixed_feat_types = ['Personality', 'Health']\n",
    "\n",
    "label_types = ['Demographic']\n",
    "\n",
    "\n",
    "variables_by_type = {\n",
    "    'Demographics': ['Age', 'Height', 'Weight', 'Number of siblings', \n",
    "                     'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type',\n",
    "                     'Home Type'],\n",
    "    'Finance': ['Finances', 'Shopping centres', 'Branded clothing', \n",
    "                'Entertainment spending', 'Spending on looks', \n",
    "                'Spending on gadgets', 'Spending on healthy eating'],\n",
    "    'Health': ['Smoking', 'Alcohol', 'Healthy eating'],\n",
    "    'Interests': ['History', 'Psychology', 'Politics', 'Mathematics', \n",
    "                  'Physics', 'Internet', 'PC', 'Economy Management', \n",
    "                  'Biology', 'Chemistry', 'Reading', 'Geography', \n",
    "                  'Foreign languages', 'Medicine', 'Law', 'Cars', \n",
    "                  'Art exhibitions', 'Religion', 'Countryside, outdoors', \n",
    "                  'Dancing', 'Musical instruments', 'Writing', 'Passive sport', \n",
    "                  'Active sport', 'Gardening', 'Celebrities', 'Shopping', \n",
    "                  'Science and technology', 'Theatre', 'Fun with friends', \n",
    "                  'Adrenaline sports', 'Pets'],\n",
    "    'Movies': ['Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', \n",
    "               'Sci-fi', 'War', 'Fantasy/Fairy tales', 'Animated', \n",
    "               'Documentary', 'Western', 'Action'],\n",
    "    'Music': ['Music', 'Slow songs or fast songs', 'Dance', 'Folk', \n",
    "              'Country', 'Classical music', 'Musical', 'Pop', 'Rock', \n",
    "              'Metal or Hardrock', 'Punk', 'Hiphop, Rap', 'Reggae, Ska', \n",
    "              'Swing, Jazz', 'Rock n roll', 'Alternative', 'Latino', \n",
    "              'Techno, Trance', 'Opera'],\n",
    "    'Personality': ['Daily events', 'Prioritising workload', \n",
    "                    'Writing notes', 'Workaholism', 'Thinking ahead', \n",
    "                    'Final judgement', 'Reliability', 'Keeping promises', \n",
    "                    'Loss of interest', 'Friends versus money', 'Funniness', \n",
    "                    'Fake', 'Criminal damage', 'Decision making', 'Elections', \n",
    "                    'Self-criticism', 'Judgment calls', 'Hypochondria', \n",
    "                    'Empathy', 'Eating to survive', 'Giving', \n",
    "                    'Compassion to animals', 'Borrowed stuff', \n",
    "                    'Loneliness', 'Cheating in school', 'Health', \n",
    "                    'Changing the past', 'God', 'Dreams', 'Charity', \n",
    "                    'Number of friends', 'Punctuality', 'Lying', 'Waiting', \n",
    "                    'New environment', 'Mood swings', 'Appearence and gestures', \n",
    "                    'Socializing', 'Achievements', 'Responding to a serious letter', \n",
    "                    'Children', 'Assertiveness', 'Getting angry', \n",
    "                    'Knowing the right people', 'Public speaking', \n",
    "                    'Unpopularity', 'Life struggles', 'Happiness in life', \n",
    "                    'Energy levels', 'Small - big dogs', 'Personality', \n",
    "                    'Finding lost valuables', 'Getting up', 'Interests or hobbies', \n",
    "                    \"Parents' advice\", 'Questionnaires or polls', 'Internet usage'],\n",
    "    'Phobias': ['Flying', 'Storm', 'Darkness', 'Heights', 'Spiders', 'Snakes', \n",
    "                'Rats', 'Ageing', 'Dangerous dogs', 'Fear of public speaking']\n",
    "}\n",
    "\n",
    "labels = variables_by_type['Demographics']\n",
    "\n",
    "try:\n",
    "    random_seed = min([StudentID1, StudentID2])\n",
    "except NameError:\n",
    "    random_seed = StudentID1\n",
    "\n",
    "def which_featgroups():\n",
    "    np.random.seed(random_seed)\n",
    "    these_entertainments = np.random.choice(var_entertainment_feat_types, 2, replace=False).tolist()\n",
    "    these_personal = np.random.choice(var_personal_feat_types, 1, replace=False).tolist()\n",
    "    these_types = fixed_feat_types + these_personal + these_entertainments\n",
    "    print('*** THESE ARE THE SELECTED TYPE OF VARIABLES:')\n",
    "    for k in these_types:\n",
    "        print(f'{k}')\n",
    "    print('*************************************')\n",
    "    return these_types\n",
    "\n",
    "def which_features(these_types):\n",
    "    np.random.seed(random_seed)\n",
    "    these_features = []\n",
    "    for type in these_types:\n",
    "        if type != 'Personality':\n",
    "            these_features += variables_by_type[type]\n",
    "        else:\n",
    "            these_features += np.random.choice(variables_by_type[type], \n",
    "                                               int(2 * (len(variables_by_type[type]) / 3)), \n",
    "                                               replace=False).tolist()\n",
    "    print('*** THESE ARE THE SELECTED FEATURES:')\n",
    "    for ft in these_features:\n",
    "        print(f'{ft}')\n",
    "    print('*************************************')\n",
    "    return these_features\n",
    "\n",
    "these_types = which_featgroups()\n",
    "these_features = which_features(these_types)\n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c9090-6065-4f25-bdc3-1b3cdad6c083",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "In the following cell, import all the modules you think are necessary for doing the homework, **among the ones listed and used during the laboratories of the course**.\n",
    "No extra modules are allowed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30ad703d-e6d8-4239-8222-7aa91fbda3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import scipy\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1143d0-e6a8-43dd-8bf8-0363842bac60",
   "metadata": {},
   "source": [
    "## Exercise 1. Preparing the Dataset\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. load the dataset \"_responses_hw.csv_\";\n",
    "2. create a working dataframe extracting from _responses_hw.csv_ the columns corresponding to the variables in _these_features_, and randomly selecting 2/3 of the rows. Let us call this dataframe _X_df_;\n",
    "3. analyze the obtained dataframe and performing cleansing/encoding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbf103f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# # plt.figure()\n",
    "# # data_df[labels[1]].plot()\n",
    "\n",
    "# print(data_df[labels[1]].mode())\n",
    "# c = Counter(data_df[labels[1]])\n",
    "# print(c.total())\n",
    "# print(c)\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(c.keys(), c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e74cf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "data_df = pd.read_csv(\"responses_hw.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b2d477-033c-40fd-b31e-f7c03ddb2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-3\n",
    "categorical = ['Smoking', 'Alcohol','Punctuality','Lying','Internet usage', 'Education', 'Gender', 'Hand', 'Only child', 'Home Town Type', 'Home Type']\n",
    "\n",
    "howToMap = {\n",
    "    'Smoking':          {'never smoked':    0,  'tried smoking':                    1,  'former smoker':    -1,  'current smoker':          3},\n",
    "    'Alcohol':          {'never':           0,  'social drinker':                   1,  'drink a lot':      2}, \n",
    "    'Punctuality':      {'early':           -1, 'on time':                          0,  'late':             1}, \n",
    "    'Lying':            {'never':           0,  'only to avoid hurting someone':    1,  'sometimes':        2,  'everytime it suits me':    3},\n",
    "    'Internet usage':   {'no time at all':  0,  'less than an hour a day':          1,  'few hours a day':  2,  'most of the day':          3},\n",
    "    'Education':        {\n",
    "        'currently a primary school pupil': 0, \n",
    "        'primary school': 1,\n",
    "        'secondary school': 2,\n",
    "        'college/bachelor degree': 3,\n",
    "        'masters degree': 4,\n",
    "        'doctorate degree': 5  \n",
    "    },\n",
    "    'Gender':           {'female':              -1, 'male':             1}, \n",
    "    'Hand':             {'left':                -1, 'right':            1}, \n",
    "    'Only child':       {'no':                  -1, 'yes':              1}, \n",
    "    'Home Town Type':   {'village':             -1, 'city':             1}, \n",
    "    'Home Type' :       {'block of flats':      -1, 'house/bungalow':   1},\n",
    "}\n",
    "\n",
    "\n",
    "for colName in data_df.columns:\n",
    "    colSeries = data_df[colName].copy()\n",
    "\n",
    "    colSeries.fillna(colSeries.mode()[0], inplace=True)\n",
    "    # colSeries[colSeries.isna()] = colSeries.mode()[0]\n",
    "    \n",
    "    data_df[colName] = colSeries.copy()\n",
    "\n",
    "data_fixed_df = data_df.copy()\n",
    "\n",
    "for colName, valueMapping in howToMap.items():\n",
    "    colSeries = data_fixed_df[colName].copy()\n",
    "\n",
    "    for valueToMap, sobstitution in valueMapping.items():\n",
    "        colSeries[colSeries == valueToMap] = sobstitution\n",
    "\n",
    "    data_fixed_df[colName] = colSeries.copy()\n",
    "\n",
    "    # data_fixed_df[colName] = data_fixed_df[colName].astype(float)\n",
    "\n",
    "data_fixed_df = data_fixed_df.astype(float)\n",
    "\n",
    "X_df:pd.DataFrame\n",
    "\n",
    "X_df, _ = sklearn.model_selection.train_test_split(data_fixed_df[these_features], test_size=0.33, random_state=random_seed) \n",
    "# X_df = data_fixed_df[these_features].sample(frac=2/3, random_state=random_seed)\n",
    "X_df = X_df.sort_index()\n",
    "\n",
    "print(f\"Type of data in resulting dataframe: {set(X_df.dtypes)}\")\n",
    "print('X_df:')\n",
    "display(X_df)\n",
    "print(f\"Number of NaN: {X_df.isna().sum().sum()}\")\n",
    "\n",
    "desc = X_df.describe()\n",
    "\n",
    "# display(desc) \n",
    "\n",
    "print(\"Variances is descending order:\")\n",
    "variances = pd.DataFrame(desc.loc['std'].iloc[np.argsort(-desc.loc['std', :])], columns=['std']).T\n",
    "display(variances)\n",
    "\n",
    "\n",
    "colors = np.array([cm.tab10(int(x in howToMap.keys())) for x in X_df.columns])\n",
    "\n",
    "legend_encoded_handle = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cm.tab10(i), markersize=10, label=label) for i, label in enumerate(['unmodified', 'encoded'])]\n",
    "\n",
    "# mask = [x in howToMap.keys() for x in desc.columns]\n",
    "# mask_not = [not x for x in mask]\n",
    "\n",
    "# idx_encoded = [i for i, value in enumerate(X_df.columns) if value in howToMap.keys()]\n",
    "# idx_original = [i for i, value in enumerate(X_df.columns) if value not in howToMap.keys()]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=desc.loc['mean', :], \n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(f\"Mean of the features\")\n",
    "plt.xlabel(\"i-th column\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=desc.loc['std', :], \n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(f\"Variance of the features\")\n",
    "plt.xlabel(\"i-th column\")\n",
    "plt.ylabel(\"Variance\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4058cb9-19fc-406c-8380-7e8053b43649",
   "metadata": {},
   "source": [
    "## Exercise 2. Analyzing the Variance and the PCs\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. create two new dataframes from _X_df_ applying a StandardScaler and a MinMaxscaler. Call these new dataframes as _Xstd_df_ and _Xmm_df_, respectively;\n",
    "2. compute the variance of all the features in _X_df_, _Xstd_df_, and _Xmm_df_ and **comment the results**;\n",
    "3. compute all the $n$ Principal Components (PCs) for each dataset _X_df_, _Xstd_df_, and _Xmm_df_. Then, visualize the curves of the cumulative explained variances and **comment the results**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "d0f2237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 \n",
    "mm_ranges = (0,1)\n",
    "Xstd_df = pd.DataFrame(StandardScaler().fit_transform(X_df), index=X_df.index, columns=X_df.columns)\n",
    "Xmm_df = pd.DataFrame(MinMaxScaler(mm_ranges).fit_transform(X_df), index=X_df.index, columns=X_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fe494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def plot_variances(X:pd.DataFrame, title):    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    ax0 = fig.add_subplot(211)\n",
    "    ax1 = fig.add_subplot(212)\n",
    "\n",
    "    ax0.scatter(\n",
    "        x=np.arange(X.shape[1]),\n",
    "        y=X.loc['std', :], \n",
    "        c=colors,\n",
    "    )\n",
    "    ax0.legend(handles=legend_encoded_handle)\n",
    "    ax0.set_ylabel('Variance')\n",
    "    ax0.grid()\n",
    "\n",
    "    ax1.scatter(\n",
    "        x=np.arange(X.shape[1]),\n",
    "        y=X.loc['mean', :], \n",
    "        c=colors,\n",
    "    )\n",
    "    ax1.legend(handles=legend_encoded_handle)\n",
    "    ax1.set_xlabel('Features')\n",
    "    ax1.set_ylabel('Mean')\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "X_desc = X_df.describe()\n",
    "Xmm_desc = Xmm_df.describe()\n",
    "Xstd_desc = Xstd_df.describe()\n",
    "\n",
    "plot_variances(X_desc, \"X_df\")\n",
    "plot_variances(Xmm_desc, \"Xmm_df\")\n",
    "plot_variances(Xstd_desc, \"Xstd_df\")\n",
    "\n",
    "rescaling_factors = X_desc.loc['std', :] / Xmm_desc.loc['std', :]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    np.arange(X_df.shape[1]),\n",
    "    rescaling_factors,\n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(\"Scaling factor in MinMax scaling\")\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Scaling Factor')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762be74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = X_df.max() - X_df.min()\n",
    "\n",
    "different_delimiters = X_desc.loc[['min', 'max'], (X_desc.loc['min', :] != X_desc.loc['min', :].mode()[0]) | (X_desc.loc['max', :] != X_desc.loc['max', :].mode()[0])].T\n",
    "different_delimiters['range'] = diff\n",
    "\n",
    "print(f\"Features with differet ranges: {different_delimiters[different_delimiters['range'] != 4].index.tolist()}\")\n",
    "display(different_delimiters)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83793299",
   "metadata": {},
   "source": [
    "In the original dataframe **X_df** in the first figure, more or less all variances have the same order of magnitude, so when we will compute the PCA, our results won't be influenced much by the values of the features. But it's not garanteeded in general since a feature with high variance will absorb all information.\n",
    "\n",
    "When we apply *Standard Scaling*, this problem is absent, so all the features contributes equally to the PCA analysis regardless of their numerical ranges.\n",
    "\n",
    "And we do in fact get that $\\forall col \\in Xstd\\_df.columns$:\n",
    "\\begin{align*}\n",
    "Var(Xstd\\_df[col]) &\\approx 1\\\\\n",
    "E(Xstd\\_df[col]) &\\approx 0\n",
    "\\end{align*}\n",
    "In the last dataframe **Xmm_df** the *MinMax Scaling* let us remove the problem of features with high numerical ranges, the resulting variances will be scaled based on the maximum and minimum value of that feature.\n",
    "Such that $\\forall col \\in Xmm\\_df.columns$:\n",
    "\\begin{align*}\n",
    "max(Xmm\\_df[col]) = 1\\\\\n",
    "min(Xmm\\_df[col]) = 0\\\\\n",
    "\\end{align*}\n",
    "So $Var(Xmm\\_df[col])$ will be the rescaled version of the original variance, but we don't have any control on $E(Xmm\\_df[col])$ and for this analysis it's not relevant.\n",
    "\n",
    "In this particular dataset almost all features have similar numerical ranges so the result of this scaling technique will maintain the distribution of variance of the original data, but there are some features who does not have a minimum of *1* and a maximum of *5*. Almost all of them are coming from how we encoded those values, the only two exceptions are *\"Fun with friends\"* wich is not encoded and has a minimum of *2*, and *\"Smoking\"*, an encoded features, that even if it has diffrent delimiters, the range that it lives in is the same as the usual case of *4*.\n",
    "\n",
    "In fact, observing the last figure titled *\"Scaling factor in MinMax scaling\"*, we can observe how having different ranges brings different scaling coefficients. And these anomalous columns are the only ones that change their relative variances between the variances in *X\\_df* and *Xmm\\_df*.\n",
    "\n",
    "So we can perform an additional rescaling before applying *MinMax Scaling* again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c64104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df = X_df*(rescaling_factors.mode()[0]/rescaling_factors)\n",
    "\n",
    "Xmm_df = pd.DataFrame(MinMaxScaler(mm_ranges).fit_transform(X_scaled_df), index=X_df.index, columns=X_df.columns)\n",
    "\n",
    "Xscaled_desc = X_scaled_df.describe()\n",
    "Xmm_desc = Xmm_df.describe()\n",
    "\n",
    "rescaling_factors_after = Xscaled_desc.loc['std', :] / Xmm_desc.loc['std', :]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    np.arange(X_df.shape[1]),\n",
    "    rescaling_factors_after,\n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(\"Scaling factor in MinMax scaling\")\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Scaling Factor')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "fig.suptitle(\"Distribution of variance\")\n",
    "\n",
    "ax0 = fig.add_subplot(211)\n",
    "ax1 = fig.add_subplot(212)\n",
    "\n",
    "ax0.set_title(\"Xscaled_df\")\n",
    "ax0.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=Xscaled_desc.loc['std', :], \n",
    "    c=colors,\n",
    ")\n",
    "ax0.legend(handles=legend_encoded_handle)\n",
    "ax0.set_ylabel('Variance')\n",
    "ax0.grid()\n",
    "\n",
    "ax1.set_title(\"Xmm_df\")\n",
    "ax1.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=Xmm_desc.loc['std', :], \n",
    "    c=colors,\n",
    ")\n",
    "ax1.legend(handles=legend_encoded_handle)\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Variance')\n",
    "ax1.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d17f1c",
   "metadata": {},
   "source": [
    "Now all features are scaled in the range *[0, 1]* and we were able to maintain the behavior of the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181358a6-6bb0-4f22-b395-34e3757fa16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "pca_x = PCA(random_state=random_seed)\n",
    "pca_x_std = PCA(random_state=random_seed)\n",
    "pca_x_mm = PCA(random_state=random_seed)\n",
    "\n",
    "pca_x.fit(X_df)\n",
    "pca_x_std.fit(Xstd_df)\n",
    "pca_x_mm.fit(Xmm_df)\n",
    "\n",
    "Y_x = pca_x.transform(X_df)\n",
    "Y_x_std = pca_x_std.transform(Xstd_df)\n",
    "Y_x_mm = pca_x_mm.transform(Xmm_df)\n",
    "\n",
    "def plot_explained_variance_ratio(pca:PCA, title:str):    \n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(pca.n_features_in_), np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.title(title)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(ticks=np.arange(0, pca.n_features_in_, 10), \n",
    "            labels=[f'PC{i + 1}' for i in range(0, pca.n_features_in_, 10)])\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_explained_variance_ratio(pca_x, 'Responses')\n",
    "plot_explained_variance_ratio(pca_x_std, 'Responses with standardization')\n",
    "plot_explained_variance_ratio(pca_x_mm, 'Responses with minmax scaling')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc230ffa",
   "metadata": {},
   "source": [
    "As highlighted in the previous point, the plot of cumulative explained variance from **X_df** and from **Xmm_df** are similar because the *MinMax* scaling had little effect on the distribution of total variance, but for **Xstd_df** the are some differences, for example with *11* principal components it does not quite reach *40%* of explained variance, while the other two dataframes does reach it.\n",
    "\n",
    "An important observation is that in all cases the first principal component is more important than the others, hence why the graph does not starts at height close to *0*. And as we consider more *PC* their importance decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1771a-09b2-41bc-bc4c-7e3f6f530021",
   "metadata": {},
   "source": [
    "## Exercise 3. Dimensionality Reduction and PC Interpretation\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two dataframes _Xstd_df_, and _Xmm_df_, compute a new PCA for performing a dimensionality reduction with respect to $m$ dimensions. The value of $m$ must be $$m = \\min\\{m', 5\\}\\,,$$ where $m'$ is the value required for obtaining $33\\%$ of the total variance.\n",
    "2. For both the cases, visualize all the PCs and give a name/interpretation to them. **Comment and motivate your interpretations**. If possible, **compare the differences among the results obtained** for _Xstd_df_ and _Xmm_df_.\n",
    "3. Perform the score graph for both the cases (_std_ and _mm_). If $m>3$, plot the score graph with respect to the first 3 PCs. All the **plots must show the names of the PCs on the axes** for better understanding the results.\n",
    "4. **Optional:** plot more score graphs, coloring the dots with respect to any label in the list _labels_ that you believe can be interesting. **Comment and analyze this optional plots**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6fe9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def perform_point_3_1(X_df:pd.DataFrame, pca:PCA):\n",
    "    a = np.cumsum(pca.explained_variance_ratio_)\n",
    "    m_prime = a[a < 0.33].shape[0] + 1\n",
    "    m = min(m_prime, 5)\n",
    "\n",
    "    pca_m = PCA(n_components=m, random_state=random_seed)\n",
    "\n",
    "    Y_m_df = pd.DataFrame(pca_m.fit_transform(X_df), columns=[f\"PC{i+1}\" for i in range(m)])\n",
    "\n",
    "    return pca_m, Y_m_df\n",
    "\n",
    "pca_x_std_m, Ystd_m_df = perform_point_3_1(Xstd_df, pca_x_std)\n",
    "pca_x_mm_m, Ymm_m_df = perform_point_3_1(Xmm_df, pca_x_mm)\n",
    "\n",
    "for i in range(min(pca_x_std_m.n_components_, pca_x_mm_m.n_components_)):\n",
    "    idx = np.argmax(pca_x_std_m.components_[i, :])\n",
    "    dir = np.sign(pca_x_std_m.components_[i, idx] * pca_x_mm_m.components_[i, idx])\n",
    "\n",
    "    if abs(dir) < 0.5:\n",
    "        dir = 1\n",
    "    pca_x_mm_m.components_[i,:] *= dir\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0de08",
   "metadata": {},
   "source": [
    "The *for* in the last lines is to make sure that at least the first *PCs* will have the same verse.\n",
    "\n",
    "Of course this does not change the result of what the *PCA* picks up, only how to interpret the *PC* that gets inverted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a01cd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# cat = labels[4]\n",
    "# Y = Ystd_m_df.copy()\n",
    "# Y[cat] = data_df.loc[X_df.index, cat].values\n",
    "# _ = sns.pairplot(Y, hue=cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.a\n",
    "def get_index_categories(cmap_, cols, variables_by_type, these_types):\n",
    "    norm = plt.Normalize(0, len(these_types))\n",
    "    colors = []\n",
    "    for col in cols:\n",
    "        for i, t in enumerate(these_types):\n",
    "            var_list = variables_by_type[t]\n",
    "\n",
    "            if col in var_list:\n",
    "                colors.append(cmap_(norm(i)))\n",
    "                break\n",
    "    return colors\n",
    "\n",
    "def create_custom_legend(cmap_, these_types): \n",
    "    norm = plt.Normalize(0, len(these_types)) \n",
    "    legend_handles = [] \n",
    "    for i, t in enumerate(these_types): \n",
    "        legend_handles.append(plt.Line2D([0], [0], color=cmap_(norm(i)), lw=4, label=t))\n",
    "    \n",
    "    return legend_handles\n",
    "\n",
    "def plot_principal_components(pca:PCA, columns, title:str, pc_names:list[str], cmap_=cm.gist_ncar):\n",
    "    pcmax = pca.components_.max()*1.1\n",
    "    pcmin = pca.components_.min()*1.1\n",
    "\n",
    "    eps = np.sqrt(1/pca.n_features_in_)\n",
    "\n",
    "    \n",
    "    for i in range(pca.n_components_):\n",
    "        plt.figure(figsize=(10,15))\n",
    "\n",
    "        idx = np.argsort(pca.components_[i, :])\n",
    "\n",
    "        # plt.barh(np.arange(0, pca_x.n_features_in_, step) - bar_height_std/2, pca_x.components_[i, :][idx][::step], height=bar_height_mm, label='Normal')\n",
    "        plt.barh(np.arange(pca.n_features_in_), pca.components_[i, :][idx], color=get_index_categories(cmap_, columns[idx], variables_by_type, these_types))\n",
    "        plt.axvline(x= eps, color='red', linestyle='--', linewidth=2)\n",
    "        plt.axvline(x=-eps, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "        plt.yticks(ticks=np.arange(pca.n_features_in_), labels=columns[idx].tolist()) \n",
    "        plt.xlim((pcmin, pcmax))\n",
    "        plt.title(title + f', responses PC{i+1}: {pc_names[i]}, {100*pca.explained_variance_ratio_[i]:.2f}% of total variance')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        legend_handles = create_custom_legend(cmap_, these_types) \n",
    "        plt.legend(handles=legend_handles, title=\"Feature Types\")\n",
    "        plt.show()\n",
    "\n",
    "        # display(f\"High values:{columns[pca.components_[i,:] > eps].tolist()}\")\n",
    "        # display(f\"Low values:{columns[pca.components_[i,:] < -eps].tolist()}\")\n",
    "\n",
    "pc_names_std = ['Cultured/Artistic', 'Delicate', 'Dynamic/Adaptable', 'Rebellious', 'Tech Oriented']\n",
    "pc_names_mm = ['Cultured/Artistic', 'Delicate', 'Dynamic/Adaptable', 'Social/Well-connected', 'Independent']\n",
    "\n",
    "plot_principal_components(pca_x_std_m, Xstd_df.columns, \"Standardized\", pc_names_std)\n",
    "# plot_principal_components(pca_x_mm_m, Xmm_df.columns, \"MinMax\", pc_names_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd32cf",
   "metadata": {},
   "source": [
    "Standarized dataframe:\n",
    "- PC1: this group of people is a mix of artistic and musical individuals who value culture and personal growth.\n",
    "- PC2: these individuals are more frightened and like to take care of themselves whether being it dressing or eating.\n",
    "- PC3: this type of people like fast-paced environments and enjoy making new connections, but they hate fear-inducing situations\n",
    "- PC4: here indpendence is key, we can identify people who dislike following rules and love going to the bar to drink and socialize.\n",
    "- PC5: individuals more oriented to technological fields who avoid politics and ecomomy in discussions.\n",
    "\n",
    "Minmax dataframe:\n",
    "- PC1: we identify people more oriented to culture and artistic expressions.\n",
    "- PC2: people who dislike being in fearful situations and likes to take care of themselves.\n",
    "- PC3: these people like dynamic environments and prefer active and engaging activites.\n",
    "- PC4: individuals that keeps up with current events, and enjoy social activities.\n",
    "- PC5: people who enjoy breaking norms and rules while maintaining a strong connection with animals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.b\n",
    "def plot_principal_components_comparisons(pca_x_std_m:PCA, pca_x_mm_m:PCA, columns:list[str]):\n",
    "    bar_space = 0.4\n",
    "    bar_height = 0.2\n",
    "\n",
    "    eps = np.sqrt(1/pca_x_std_m.n_features_in_)\n",
    "\n",
    "    m = min(pca_x_std_m.n_components_, pca_x_mm_m.n_components_)\n",
    "\n",
    "    pcmax = max(pca_x_std_m.components_[:m, :].max(), pca_x_mm_m.components_[:m, :].max())\n",
    "    pcmax *= 1+np.sign(pcmax)*0.1\n",
    "    pcmin = min(pca_x_std_m.components_[:m, :].min(), pca_x_mm_m.components_[:m, :].min())\n",
    "    pcmin *= 1-np.sign(pcmin)*0.1\n",
    "\n",
    "    for i in range(m):\n",
    "        plt.figure(figsize=(10,15)) \n",
    "        idx = np.argsort(pca_x_std_m.components_[i, :])\n",
    "\n",
    "        plt.barh(np.arange(pca_x_std_m.n_features_in_) + bar_space/2, pca_x_std_m.components_[i, idx], height=bar_height, label='Standardized')\n",
    "        plt.barh(np.arange(pca_x_mm_m.n_features_in_)  - bar_space/2, pca_x_mm_m.components_[i, idx], height=bar_height, label='Min-Max Scaled')\n",
    "        \n",
    "        plt.axvline(x= eps, color='red', linestyle='--', linewidth=2)\n",
    "        plt.axvline(x=-eps, color='red', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.xlim((pcmin, pcmax))\n",
    "        \n",
    "        plt.yticks(ticks=np.arange(pca_x_std_m.n_features_in_), labels=columns[idx].tolist()) \n",
    "        plt.title(f'Responses PC{i+1}')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        plt.legend()\n",
    "\n",
    "        plt.show()\n",
    "\n",
    "plot_principal_components_comparisons(pca_x_std_m, pca_x_mm_m, X_df.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e4f93",
   "metadata": {},
   "source": [
    "We can see that in the first two principal components there are some differences, but both *Xstd_df* and *Xmm_df* capture the same groups.\n",
    "\n",
    "Starting from the third component, their meaning start to diverge between the analyses. The *PC3* can still be connected to the same type of people since the features with the highest and lowest values are basically the same. But from *PC4* we can no longer say that they represent the same individuals, this problem is even more highlighted in *PC5*.\n",
    "\n",
    "This happens because of two motives:\n",
    "\n",
    "1. We used different scaling techniques, so the information carried by the features themselves is different\n",
    "1. As we inspect deeper into the PCs, their explained variance becomes lower and lower, for example *PC1* explain about *8%* of the total variance and *PC5* just about *3%* of total variance. So the information that they represent is less and they should be contribute less in the decisions arised from this analysis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "7edb6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_graph2D(ax:plt.Axes, Y:np.ndarray, title:str, ax_names:list[str], colors, s:float=20, all_same_color=False, mappable:cm.ScalarMappable=None):\n",
    "    if all_same_color:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], color=colors, s=s)\n",
    "    else:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], c=colors, s=s)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(ax_names[0])\n",
    "    ax.set_ylabel(ax_names[1])\n",
    "\n",
    "    # if mappable is not None:\n",
    "    #     fig.colorbar(mappable=mappable, ax=ax)\n",
    "\n",
    "    ax.grid()\n",
    "    return ax\n",
    "\n",
    "def score_graph3D(ax:plt.Axes, Y:np.ndarray, title:str, ax_names:list[str], colors, s:float=20, all_same_color=False, mappable:cm.ScalarMappable=None):\n",
    "    if all_same_color:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], color=colors, s=s)\n",
    "    else:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], c=colors, s=s)\n",
    "        \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(ax_names[0])\n",
    "    ax.set_ylabel(ax_names[1])\n",
    "    ax.set_zlabel(ax_names[2])\n",
    "    \n",
    "    # if mappable is not None:\n",
    "    #     fig.colorbar(mappable=mappable, ax=ax)\n",
    "    \n",
    "    ax.grid()\n",
    "    return ax\n",
    "\n",
    "def isolated_score_graph(Y:np.ndarray, title:str, ax_names:list[str], colors, s:float, all_same_color=False, mappable:cm.ScalarMappable=None, handles=None, mode:str='2d'):\n",
    "    if mode not in ['2d', '3d']:\n",
    "        raise ValueError(\"'mode' must be '2d' or '3d'\")\n",
    "    \n",
    "    scoregraph = plt.figure()\n",
    "\n",
    "    if mode == '2d':\n",
    "        ax = scoregraph.add_subplot()\n",
    "        score_graph2D(ax, Y, title, ax_names, colors, s, all_same_color, mappable)\n",
    "    else:\n",
    "        ax = scoregraph.add_subplot(projection='3d')\n",
    "        score_graph3D(ax, Y, title, ax_names, colors, s, all_same_color, mappable)\n",
    "\n",
    "    if all_same_color == False:\n",
    "        if mappable is not None:\n",
    "            scoregraph.colorbar(ax=ax, mappable=mappable)\n",
    "        else:\n",
    "            scoregraph.legend(loc='center right', bbox_to_anchor=(1, 0.5), handles=handles)\n",
    "            # scoregraph.legend(handles=handles)\n",
    "    return scoregraph, ax\n",
    "\n",
    "def create_colors(values:pd.Series, cat:str, cmap_continuous = cm.viridis, cmap_discrete = cm.rainbow):\n",
    "    if cat in ['Age', 'Height', 'Weight', 'Number of siblings']:\n",
    "        Q1 = values.quantile(0.25) \n",
    "        Q3 = values.quantile(0.75) \n",
    "        IQR = Q3 - Q1 \n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR \n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        norm = plt.Normalize(vmin=max(lower_bound, np.min(values)), vmax=min(upper_bound, np.max(values))) \n",
    "\n",
    "        cmap_ = cmap_continuous\n",
    "        # norm = plt.Normalize(vmin=np.min(values), vmax=np.max(values)) \n",
    "        mappable = cm.ScalarMappable(norm=norm, cmap=cmap_)\n",
    "        handles = None\n",
    "    else:\n",
    "        mappable = None\n",
    "        norm = plt.Normalize(vmin=np.min(values), vmax=np.max(values)) \n",
    "\n",
    "        cmap_ = cmap_discrete\n",
    "        handles = [Line2D([0], [0], marker='o', color='none', linestyle='None', markeredgewidth=0, markerfacecolor=cmap_(norm(val)), markersize=10, label=label) for label, val in howToMap[cat].items()]\n",
    "\n",
    "    return cmap_, norm, mappable, handles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "cmap_1 = cm.tab10\n",
    "\n",
    "isolated_score_graph(Ystd_m_df.values, \"STANDARDIZED - SCORE GRAPH\", pc_names_std, cmap_1(1), s=20, all_same_color=True, mode='2d'); plt.show()\n",
    "isolated_score_graph(Ymm_m_df.values,  \"MINMAX - SCORE GRAPH\",       pc_names_mm,  cmap_1(0), s=20, all_same_color=True, mode='2d'); plt.show()\n",
    "isolated_score_graph(Ystd_m_df.values, \"STANDARDIZED - SCORE GRAPH\", pc_names_std, cmap_1(1), s=20, all_same_color=True, mode='3d'); plt.show()\n",
    "isolated_score_graph(Ymm_m_df.values,  \"MINMAX - SCORE GRAPH\",       pc_names_mm,  cmap_1(0), s=20, all_same_color=True, mode='3d'); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48406cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "def plot_score_graphs_colored(Y:pd.DataFrame, title:str, pc_names:list[str], colors, mappable=None, handles=None):\n",
    "    m = Y.shape[1]\n",
    "\n",
    "    fig = plt.figure(figsize=(14,14))\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    axes = fig.subplots(m, m)\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if (i == m-1) | (j == 0):\n",
    "                if i == m-1:\n",
    "                    xname = pc_names[j]\n",
    "                else:\n",
    "                    xname = ''\n",
    "                if j == 0:\n",
    "                    yname = pc_names[i]\n",
    "                else:\n",
    "                    yname = ''\n",
    "            else:\n",
    "                xname =''\n",
    "                yname =''\n",
    "                \n",
    "            score_graph2D(axes[i, j], Y=Y.values[:, [j,i]], title='', ax_names=[xname, yname], colors=colors, s=5, all_same_color=False, mappable=None)\n",
    "\n",
    "    # score_graph2D(fig, axes[1], Y=Ystd_m_df.values[:, [z, y]], title=f\"PC{z} (hor.) vs PC{y} (ver.)\", ax_names=[pc_names_std[i] for i in [z, y]], colors=cmap_(norm(values)),  all_same_color=False, mappable=None)\n",
    "    # axes.append(score_graph2D(Y=Ymm_m_df.values,  title=f\"MINMAX - SCORE GRAPH, colored on {cat}\",       ax_names=pc_names_mm,  colors=cmap_(norm(values)),  all_same_color=False, mappable=mappable))\n",
    "    # axes.append(score_graph3D(Y=Ystd_m_df.values, title=f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", ax_names=pc_names_std, colors=cmap_(norm(values)),  all_same_color=False, mappable=mappable))\n",
    "    # axes.append(score_graph3D(Y=Ymm_m_df.values,  title=f\"MINMAX - SCORE GRAPH, colored on {cat}\",       ax_names=pc_names_mm,  colors=cmap_(norm(values)),  all_same_color=False, mappable=mappable))\n",
    "\n",
    "    if handles is not None:\n",
    "        # fig.legend(loc='center right', bbox_to_anchor=(1, 0.5), handles=handles)\n",
    "        fig.legend(handles=handles)\n",
    "    else:\n",
    "        fig.colorbar(ax=axes, mappable=mappable)\n",
    "    \n",
    "    return fig\n",
    "\n",
    "# for i in [1, 2, 4, 5]:\n",
    "for i in [1,2,4]:\n",
    "    cat = labels[i]\n",
    "    values = data_fixed_df.loc[X_df.index, cat].astype(float)\n",
    "    cmap_, norm, mappable, handles = create_colors(values, labels[i])\n",
    "\n",
    "    fig = plot_score_graphs_colored(Ystd_m_df, title=f\"STANDARDIZED- SCORE GRAPH, colored on {cat}\", pc_names=pc_names_std, colors=cmap_(norm(values)), mappable=mappable, handles=handles)\n",
    "\n",
    "    y = 0\n",
    "    x = 1\n",
    "    z = 2\n",
    "    \n",
    "    fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [x, y]], title=f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", ax_names=[pc_names_std[i] for i in [x, y]], colors=cmap_(norm(values)), s=20, all_same_color=False, mappable=mappable, handles=handles, mode='2d')\n",
    "    fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [z, y]], title=f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", ax_names=[pc_names_std[i] for i in [z, y]], colors=cmap_(norm(values)), s=20, all_same_color=False, mappable=mappable, handles=handles, mode='2d')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a92ac9",
   "metadata": {},
   "source": [
    "The first thing we did is to divide the labels in *2*, the first group can be seen as continous and it's composed of *'Age', 'Height', 'Weight', 'Number of siblings'* even if *'a sibling and a half'* does not exist, while the other is discrete and composed of the remaining *'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type', 'Home Type'*.\n",
    "\n",
    "In the figure of $m\\times m$ plots, the ones on the main diagonal help to see if a *PC* is correlatd to the value of the label. The ones off the main diagonal are specular, so theoretically only the upper (lower) triangular part is useful.\n",
    "\n",
    "The scoregraph based on *PC1* vs. *PC2* and *PC1* vs. *PC3* are added isolated from the rest to help with insights.\n",
    "\n",
    "We need to highlight that for the non discrete labels we applied some outlier filtering based on interquartile ranges just to make more evident the coloring of most of the population. Otherwise an outlier would have ruinded the coloring palette, for example in *'Height'* there is a single person tall *60*, and similar problem occur in *'Weight' and 'Number of siblings'*.\n",
    "\n",
    "- If we color based on *'Age'*, we can observe that there isn't any correlation between label and *PC* value\n",
    "- If we instead color on *'Weight'* or *'Height'*, we can see how more compact people (either lighter or smaller) tend to be more oriented to art and to face more fearful things and they are less adapdable. But it's not that marked of a trend.\n",
    "- When we color by *'Number of siblings'* we do not obtain any new informations\n",
    "- More interesting is the coloring based on *'Gender'*, where we can more evidently see how females and men approach art, fear and changes. Of course the division is evident but not rigid since there will always be exceptions.\n",
    "- For *'Hand', 'Education', 'Only child', 'Home Town Type', 'Home Type'* there isn't any information that can be derived.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8f0b8-37c5-403a-93ad-8abefd96e3b2",
   "metadata": {},
   "source": [
    "## Exercise 4. $k$-Means\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), run the $k$-Means for clustering the data. In particular, **use the silohuette score for identify the best value for $k\\in\\{3, \\ldots, 10\\}$**.\n",
    "2. Plot the score graphs of exercise 3.3, adding the centroids of the cluster.\n",
    "3. Observing the centroids coordinates in the PC space, **give a name/interpretation to them**, exploiting the names you assigned to the PCs. **Comment and motivate your interpretations**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7be2-1756-4e44-9bcd-56a0fa7c78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "def run_kmeans(Y)-> KMeans:\n",
    "    km_list = []\n",
    "\n",
    "    silcoeff_list = pd.DataFrame(index=range(3,11), columns=['silhouette'])\n",
    "\n",
    "    silcoeff_list.index.name = 'k'\n",
    "\n",
    "    for i in range(silcoeff_list.shape[0]):\n",
    "        km_list.append(KMeans(n_clusters=silcoeff_list.index[i], random_state=random_seed))\n",
    "        km = km_list[i]\n",
    "        km_list[i] = km.fit(Y)\n",
    "\n",
    "        silcoeff_list.iloc[i, :] = silhouette_score(Y, labels=km.fit_predict(Y))\n",
    "\n",
    "    i_best = np.argmax(silcoeff_list.iloc[:, 0])\n",
    "    k = silcoeff_list.index[i_best]\n",
    "    km = km_list[i_best]\n",
    "\n",
    "    print('****************** RESULTS OF THE SEARCH... ******************')\n",
    "    print(f'BEST SILHOUETTE SCORE: {silcoeff_list.iloc[i_best, 0]} --> k = {k}')\n",
    "    print('**************************************************************')\n",
    "\n",
    "    display(silcoeff_list.sort_values(by='silhouette', ascending=False))\n",
    "\n",
    "    return km\n",
    "\n",
    "print(\"Standard version\")\n",
    "km_std_best = run_kmeans(Ystd_m_df)\n",
    "print(\"Minmax version\")\n",
    "km_mm_best = run_kmeans(Ymm_m_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "cmap_3 = cm.tab10\n",
    "s = 20\n",
    "\n",
    "cluster_assignment_std = km_std_best.predict(Ystd_m_df)\n",
    "cluster_assignment_mm = km_mm_best.predict(Ymm_m_df)\n",
    "norm_clusters_std = plt.Normalize(vmin=np.min(cluster_assignment_std), vmax=np.max(cluster_assignment_std)) \n",
    "norm_clusters_mm = plt.Normalize(vmin=np.min(cluster_assignment_mm), vmax=np.max(cluster_assignment_mm)) \n",
    "\n",
    "\n",
    "# display(create_custom_legend(cmap_3, np.unique(cluster_assignment_std)))\n",
    "\n",
    "handles_std = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap_3(norm_clusters_std(c)), markersize=10, label=c) for c in np.unique(cluster_assignment_std)]\n",
    "handles_mm = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap_3(norm_clusters_mm(c)), markersize=10, label=c) for c in np.unique(cluster_assignment_mm)]\n",
    "\n",
    "x = 0\n",
    "y = 1\n",
    "fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [x, y]], title=f\"STANDARDIZED - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_std[i] for i in [x, y]], colors=cmap_3(norm_clusters_std(cluster_assignment_std)), s=s, all_same_color=False, mappable=None, handles=handles_std, mode='2d')\n",
    "ax.scatter(km_std_best.cluster_centers_[:, x], km_std_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "x = 0\n",
    "y = 2\n",
    "fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [x, y]], title=f\"STANDARDIZED - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_std[i] for i in [x, y]], colors=cmap_3(norm_clusters_std(cluster_assignment_std)), s=s, all_same_color=False, mappable=None, handles=handles_std, mode='2d')\n",
    "ax.scatter(km_std_best.cluster_centers_[:, x], km_std_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "x = 0\n",
    "y = 1\n",
    "fig, ax = isolated_score_graph(Y=Ymm_m_df.values[:, [x, y]], title=f\"MINMAX - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_mm[i] for i in [x, y]], colors=cmap_3(norm_clusters_mm(cluster_assignment_mm)), s=s, all_same_color=False, mappable=None, handles=handles_mm, mode='2d')\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, x], km_mm_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "x = 0\n",
    "y = 2\n",
    "fig, ax = isolated_score_graph(Y=Ymm_m_df.values[:, [x, y]], title=f\"MINMAX - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_mm[i] for i in [x, y]], colors=cmap_3(norm_clusters_mm(cluster_assignment_mm)), s=s, all_same_color=False, mappable=None, handles=handles_mm, mode='2d')\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, x], km_mm_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "\n",
    "fig, ax = isolated_score_graph(Ystd_m_df.values, f'STANDARDIZED - SCORE GRAPH, colored on cluster assignment', pc_names_std, cmap_3(norm_clusters_std(cluster_assignment_std)), s=s, all_same_color=False, mappable=None, handles=handles_std, mode='3d')\n",
    "ax.scatter(km_std_best.cluster_centers_[:, 0], km_std_best.cluster_centers_[:, 1], km_std_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = isolated_score_graph(Ymm_m_df.values, f'MINMAX - SCORE GRAPH, colored based on cluster assignment', pc_names_mm, cmap_3(norm_clusters_mm(cluster_assignment_mm)), s=s, all_same_color=False, mappable=None, handles=handles_mm, mode='3d')\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, 0], km_mm_best.cluster_centers_[:, 1], km_mm_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb169142",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "maxs_std_m = Ystd_m_df.max(axis=0) \n",
    "mins_std_m = Ystd_m_df.min(axis=0) \n",
    "\n",
    "maxs_mm_m = Ymm_m_df.max(axis=0) \n",
    "mins_mm_m = Ymm_m_df.min(axis=0) \n",
    "\n",
    "k_std = km_std_best.cluster_centers_.shape[0]\n",
    "k_mm = km_mm_best.cluster_centers_.shape[0]\n",
    "\n",
    "fig_mm, ax_mm = plt.subplots(1, k_mm, figsize=(15, 7))\n",
    "\n",
    "for ii in range(k_mm):\n",
    "    ax_mm[ii].barh(np.arange(km_mm_best.cluster_centers_.shape[1]), maxs_mm_m, color='blue', alpha=0.15)\n",
    "    ax_mm[ii].barh(np.arange(km_mm_best.cluster_centers_.shape[1]), mins_mm_m, color='blue', alpha=0.15)\n",
    "    ax_mm[ii].barh(np.arange(km_mm_best.cluster_centers_.shape[1]), km_mm_best.cluster_centers_[ii, :])\n",
    "    \n",
    "    ax_mm[ii].set_yticks(ticks=np.arange(km_mm_best.cluster_centers_.shape[1]))\n",
    "    if ii == 0:\n",
    "        ax_mm[ii].set_yticklabels(labels=[f\"{name} (PC{i+1})\" for i, name in enumerate(pc_names_mm)], rotation=60)\n",
    "    \n",
    "    ax_mm[ii].grid(visible=True, which='both')\n",
    "    ax_mm[ii].set_title(f'MinMax - CENTROID {ii+1}')\n",
    "\n",
    "\n",
    "fig_std, ax_std = plt.subplots(1, k_std, figsize=(15, 7))\n",
    "for ii in range(k_std):\n",
    "    ax_std[ii].barh(np.arange(km_std_best.cluster_centers_.shape[1]), maxs_std_m, color='blue', alpha=0.15)\n",
    "    ax_std[ii].barh(np.arange(km_std_best.cluster_centers_.shape[1]), mins_std_m, color='blue', alpha=0.15)\n",
    "    ax_std[ii].barh(np.arange(km_std_best.cluster_centers_.shape[1]), km_std_best.cluster_centers_[ii, :])\n",
    "\n",
    "    ax_std[ii].set_yticks(ticks=np.arange(km_std_best.cluster_centers_.shape[1]))\n",
    "    if ii == 0:\n",
    "        ax_std[ii].set_yticklabels(labels=[f\"{name} (PC{i+1})\" for i, name in enumerate(pc_names_std)], rotation=60)\n",
    "    \n",
    "    ax_std[ii].grid(visible=True, which='both')\n",
    "    ax_std[ii].set_title(f'Standard - CENTROID {ii+1}')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1f3d9",
   "metadata": {},
   "source": [
    "Regarding the clustering performed on **Xmm_df** in PC space we can observe that since the points are basically a cloud, the centers will be equidistant from each other, and in fact from the first two main *PC* this happens.\n",
    "\n",
    "So the achetypes of customers that we can identify are the three most diverse from each other:\n",
    "1. Likes art and it's a bit more introvert (Art Enthusiast)\n",
    "1. Does not like art and tends to be introvert (Calm Rationalist)\n",
    "1. Indifferent to art and it's extrovert (Social Realist)\n",
    "\n",
    "Of course if somone were to like art and it is extrovert, we just need to 'sum' the effect of the two interested clusters.\n",
    "\n",
    "The same thing happens for **Xstd_df**, meaning we can find the same archetype of customers. The only difference is that the numerical value associated to this centroid are different, but still maintaining similar proportion with respect to the maximum and minimum value of that feature in PC space.\n",
    "\n",
    "Why am i not talking about the other 3 *PC*?\n",
    "\n",
    "Two motives:\n",
    "- arinza\n",
    "- arunza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5313a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_names = ['Art Enthusiast', 'Calm Rationalist', 'Social Realist']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb169142",
   "metadata": {},
   "source": [
    "## Exercise 5. Cluster Evaluations\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), perform an **external evaluation** of the clustering obtained at exercise 4.1 with respect to one or more labels in the list _labels_. **Comment the results, comparing the evaluation with the interpretation you gave at exercise 4.3**. \n",
    "2. For each one of the two datasets (_std_ and _mm_), perform an **internal evaluation** of each cluster, with respect to the silohuette score. **Comment the results**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# TODO: fix me\n",
    "def external_evaluation(cluster_assignment:np.ndarray, label_series:pd.Series, title:str, label:str):\n",
    "    distribution = pd.crosstab(cluster_assignment, label_series)\n",
    "\n",
    "    distribution.index.name = 'Cluster'\n",
    "    distribution.columns.name = label\n",
    "\n",
    "    # distribution.plot(ax=ax, kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "\n",
    "    total_counts = distribution.sum(axis=1)\n",
    "\n",
    "    cmap_, norm, mappable, handles = create_colors(label_series, label, cmap_continuous=cm.rainbow)\n",
    "\n",
    "    percent = 0.8\n",
    "    width = percent/distribution.shape[1]\n",
    "    multiplier = -distribution.shape[1]/2.0 + width/percent\n",
    "\n",
    "    x = np.arange(distribution.shape[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for colName in distribution.columns:\n",
    "        offset = width*multiplier\n",
    "\n",
    "        ax.bar(\n",
    "            x + offset, \n",
    "            height=distribution[colName]/total_counts, \n",
    "            width=width,\n",
    "            color = cmap_(norm(colName))\n",
    "        )\n",
    "        multiplier += 1\n",
    "\n",
    "    if mappable is not None:\n",
    "        fig.colorbar(ax=ax, mappable=mappable, label=label)\n",
    "    else:\n",
    "        fig.legend(handles=handles, loc='center right', title=label)\n",
    "\n",
    "    ax.set_ylabel('Count')\n",
    "    ax.set_title(f'Distribution of {label} in clusters')\n",
    "    ax.set_xticks(x, centroids_names)\n",
    "\n",
    "\n",
    "for i in [0, 1, 2, 4, 5, 6]:\n",
    "    external_evaluation(cluster_assignment_std, data_fixed_df.loc[Xstd_df.index, labels[i]].astype(float), title=\"STANDARDIZED dataset\", label=labels[i])\n",
    "    # external_evaluation(cluster_assignment_mm, data_fixed_df.loc[Xmm_df.index, labels[i]].astype(float), title=\"MINMAX dataset\", label=labels[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a486d98",
   "metadata": {},
   "source": [
    "The figures are structured to highlight w.r.t. each cluster, denoted by the name of the associated centroid, the distribution of labels normalized by the total number of customers per cluster.\n",
    "\n",
    "To know if a label is characterizing of a centroid, we need to observe if the distribution of the values of a label between the clusters is notably different.\n",
    "\n",
    "For example with *Gender* we can clearly see how in **some** clusters there's more males or females, so this label **is** important, the counter example is *Hand*, where even if in a cluster almost all of them are right-handed, all of the clusters have the **same** distribution of labels, so it's **not** meaningful.\n",
    "\n",
    "Explained this, we can continue with the results.\n",
    "\n",
    "Based on the data, we can conclude that:\n",
    "- Someone that likes art, tends to have an height in the lower range of approx 165 - 175 cm, and in most of the cases it's a male\n",
    "- Peoples \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babe2f4-f800-4f51-9c97-6059d4ca12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "def internal_evaluation(kmean:KMeans, Y, cluster_assignment):\n",
    "    n_clusters = kmean.cluster_centers_.shape[0]\n",
    "\n",
    "    sil_sample_series = silhouette_samples(Y, labels=cluster_assignment)\n",
    "    y_lower = 10\n",
    "\n",
    "    plt.figure()\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = np.sort(sil_sample_series[cluster_assignment == i])\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        plt.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        y_lower = y_upper +10\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "internal_evaluation(km_std_best, Ystd_m_df, cluster_assignment_std)\n",
    "internal_evaluation(km_mm_best, Ymm_m_df, cluster_assignment_mm)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
