{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6347f1d2-5cb0-4032-b637-4730e9b9383a",
   "metadata": {},
   "source": [
    "# Computational Linear Algebra: PCA Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a905c56f-029b-4d04-9e68-739362350bd8",
   "metadata": {},
   "source": [
    "## Initialization:\n",
    "Fill the missing values in this text box and in the following code-cell.\n",
    "\n",
    "**Academic Year:** 2024/2025\n",
    "\n",
    "### Team Members (Alphabetical Order):\n",
    "1. Licalzi, Nunzio (344860);\n",
    "2. Vercellone, Romeo (341967)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c068e99-0c8c-433f-be0b-f41534dc8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "StudentID1 = 341967  # <-------- Fill in the missing value\n",
    "StudentID2 = 344860  # <-------- Fill in the missing value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a24f22b-a92f-4898-b26d-e4ab6d49c3d9",
   "metadata": {},
   "source": [
    "## Starting Code-Cell \n",
    "### Attention: DO NOT CHANGE THE CODE INSIDE THE FOLLOWING CELL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84734de0-62ad-4c39-b4b4-886696d3a3f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################\n",
    "############## DO NOT CHANGE THE CODE IN THIS CELL #################\n",
    "####################################################################\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "var_entertainment_feat_types = ['Interests', 'Movies', 'Music']\n",
    "var_personal_feat_types = ['Finance', 'Phobias']\n",
    "fixed_feat_types = ['Personality', 'Health']\n",
    "\n",
    "label_types = ['Demographic']\n",
    "\n",
    "\n",
    "variables_by_type = {\n",
    "    'Demographics': ['Age', 'Height', 'Weight', 'Number of siblings', \n",
    "                     'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type',\n",
    "                     'Home Type'],\n",
    "    'Finance': ['Finances', 'Shopping centres', 'Branded clothing', \n",
    "                'Entertainment spending', 'Spending on looks', \n",
    "                'Spending on gadgets', 'Spending on healthy eating'],\n",
    "    'Health': ['Smoking', 'Alcohol', 'Healthy eating'],\n",
    "    'Interests': ['History', 'Psychology', 'Politics', 'Mathematics', \n",
    "                  'Physics', 'Internet', 'PC', 'Economy Management', \n",
    "                  'Biology', 'Chemistry', 'Reading', 'Geography', \n",
    "                  'Foreign languages', 'Medicine', 'Law', 'Cars', \n",
    "                  'Art exhibitions', 'Religion', 'Countryside, outdoors', \n",
    "                  'Dancing', 'Musical instruments', 'Writing', 'Passive sport', \n",
    "                  'Active sport', 'Gardening', 'Celebrities', 'Shopping', \n",
    "                  'Science and technology', 'Theatre', 'Fun with friends', \n",
    "                  'Adrenaline sports', 'Pets'],\n",
    "    'Movies': ['Movies', 'Horror', 'Thriller', 'Comedy', 'Romantic', \n",
    "               'Sci-fi', 'War', 'Fantasy/Fairy tales', 'Animated', \n",
    "               'Documentary', 'Western', 'Action'],\n",
    "    'Music': ['Music', 'Slow songs or fast songs', 'Dance', 'Folk', \n",
    "              'Country', 'Classical music', 'Musical', 'Pop', 'Rock', \n",
    "              'Metal or Hardrock', 'Punk', 'Hiphop, Rap', 'Reggae, Ska', \n",
    "              'Swing, Jazz', 'Rock n roll', 'Alternative', 'Latino', \n",
    "              'Techno, Trance', 'Opera'],\n",
    "    'Personality': ['Daily events', 'Prioritising workload', \n",
    "                    'Writing notes', 'Workaholism', 'Thinking ahead', \n",
    "                    'Final judgement', 'Reliability', 'Keeping promises', \n",
    "                    'Loss of interest', 'Friends versus money', 'Funniness', \n",
    "                    'Fake', 'Criminal damage', 'Decision making', 'Elections', \n",
    "                    'Self-criticism', 'Judgment calls', 'Hypochondria', \n",
    "                    'Empathy', 'Eating to survive', 'Giving', \n",
    "                    'Compassion to animals', 'Borrowed stuff', \n",
    "                    'Loneliness', 'Cheating in school', 'Health', \n",
    "                    'Changing the past', 'God', 'Dreams', 'Charity', \n",
    "                    'Number of friends', 'Punctuality', 'Lying', 'Waiting', \n",
    "                    'New environment', 'Mood swings', 'Appearence and gestures', \n",
    "                    'Socializing', 'Achievements', 'Responding to a serious letter', \n",
    "                    'Children', 'Assertiveness', 'Getting angry', \n",
    "                    'Knowing the right people', 'Public speaking', \n",
    "                    'Unpopularity', 'Life struggles', 'Happiness in life', \n",
    "                    'Energy levels', 'Small - big dogs', 'Personality', \n",
    "                    'Finding lost valuables', 'Getting up', 'Interests or hobbies', \n",
    "                    \"Parents' advice\", 'Questionnaires or polls', 'Internet usage'],\n",
    "    'Phobias': ['Flying', 'Storm', 'Darkness', 'Heights', 'Spiders', 'Snakes', \n",
    "                'Rats', 'Ageing', 'Dangerous dogs', 'Fear of public speaking']\n",
    "}\n",
    "\n",
    "labels = variables_by_type['Demographics']\n",
    "\n",
    "try:\n",
    "    random_seed = min([StudentID1, StudentID2])\n",
    "except NameError:\n",
    "    random_seed = StudentID1\n",
    "\n",
    "def which_featgroups():\n",
    "    np.random.seed(random_seed)\n",
    "    these_entertainments = np.random.choice(var_entertainment_feat_types, 2, replace=False).tolist()\n",
    "    these_personal = np.random.choice(var_personal_feat_types, 1, replace=False).tolist()\n",
    "    these_types = fixed_feat_types + these_personal + these_entertainments\n",
    "    print('*** THESE ARE THE SELECTED TYPE OF VARIABLES:')\n",
    "    for k in these_types:\n",
    "        print(f'{k}')\n",
    "    print('*************************************')\n",
    "    return these_types\n",
    "\n",
    "def which_features(these_types):\n",
    "    np.random.seed(random_seed)\n",
    "    these_features = []\n",
    "    for type in these_types:\n",
    "        if type != 'Personality':\n",
    "            these_features += variables_by_type[type]\n",
    "        else:\n",
    "            these_features += np.random.choice(variables_by_type[type], \n",
    "                                               int(2 * (len(variables_by_type[type]) / 3)), \n",
    "                                               replace=False).tolist()\n",
    "    print('*** THESE ARE THE SELECTED FEATURES:')\n",
    "    for ft in these_features:\n",
    "        print(f'{ft}')\n",
    "    print('*************************************')\n",
    "    return these_features\n",
    "\n",
    "these_types = which_featgroups()\n",
    "these_features = which_features(these_types)\n",
    "\n",
    "np.random.seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090c9090-6065-4f25-bdc3-1b3cdad6c083",
   "metadata": {},
   "source": [
    "## Importing Modules\n",
    "\n",
    "In the following cell, import all the modules you think are necessary for doing the homework, **among the ones listed and used during the laboratories of the course**.\n",
    "No extra modules are allowed for reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "30ad703d-e6d8-4239-8222-7aa91fbda3e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ...\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# import scipy\n",
    "\n",
    "import sklearn.model_selection\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "%matplotlib widget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1143d0-e6a8-43dd-8bf8-0363842bac60",
   "metadata": {},
   "source": [
    "## Exercise 1. Preparing the Dataset\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. load the dataset \"_responses_hw.csv_\";\n",
    "2. create a working dataframe extracting from _responses_hw.csv_ the columns corresponding to the variables in _these_features_, and randomly selecting 2/3 of the rows. Let us call this dataframe _X_df_;\n",
    "3. analyze the obtained dataframe and performing cleansing/encoding operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cbf103f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "\n",
    "\n",
    "# # plt.figure()\n",
    "# # data_df[labels[1]].plot()\n",
    "\n",
    "# print(data_df[labels[1]].mode())\n",
    "# c = Counter(data_df[labels[1]])\n",
    "# print(c.total())\n",
    "# print(c)\n",
    "\n",
    "\n",
    "# plt.figure()\n",
    "# plt.scatter(c.keys(), c.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e74cf926",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "data_df = pd.read_csv(\"responses_hw.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7365dc4",
   "metadata": {},
   "source": [
    "## <b>DATA EXPLORATION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f46a8",
   "metadata": {},
   "source": [
    "We can plot each column of the dataframe to count for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150092b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 25))\n",
    "ax.barh((missing := data_df.isna().sum()[data_df.isna().sum() > 0].sort_values(ascending=True)).index, missing, color='skyblue')\n",
    "ax.axvline(missing.mean(), color='red', linestyle='--', label=f'Average: {missing.mean():.2f}')\n",
    "ax.set_title('Features with missing values in the dataset')\n",
    "ax.set_ylabel('features with missing values'), ax.set_xlabel('number of missing values')\n",
    "ax.legend(loc='lower right')\n",
    "ax.set_xticks(range(missing.max()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6b7c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*data_df.isna().sum()[data_df.isna().sum()==0].sort_values(ascending=True).index, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8097123",
   "metadata": {},
   "source": [
    "As we can see, most of the columns have missing values. The number of missing values in each column is between $0$ and $20$ with an average of around $4.2$, moreover, there are only 6 columns without missing values <em>(Snakes, Eating to survive, Dreams, Number of friends, Internet usage, Spending on gadgets).</em> <br>\n",
    "\n",
    "we have to dealt with missing values in some way, we opted to fill missing values with the most frequent value in each column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8123f8",
   "metadata": {},
   "source": [
    "We can also identify the columns that are categorical and those that are numerical. <br>\n",
    "Those columns should be encoded using a technique called <b>Label Encoding</b> since even if the values are categorical they can be ordered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc63a98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*[col for col in data_df.columns if data_df[col].dtype == 'object'], sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214671a",
   "metadata": {},
   "source": [
    "We can now fill the missing values with the most frequent value. <br>\n",
    "We opted to use the mode instead of the average or mean because there may be hard to spot outliers that might affect the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b1b2d477-033c-40fd-b31e-f7c03ddb2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "for colName in data_df.columns:\n",
    "    colSeries = data_df[colName].copy()\n",
    "    colSeries.fillna(colSeries.mode()[0], inplace=True)\n",
    "    # colSeries[colSeries.isna()] = colSeries.mode()[0]\n",
    "    \n",
    "    data_df[colName] = colSeries.copy()\n",
    "\n",
    "data_fixed_df = data_df.copy()\n",
    "\n",
    "# Now, if we have done everything correctly, there will be no missing values in the dataframe.\n",
    "if any(data_fixed_df.isna().sum() != 0):\n",
    "    raise ValueError(\"missing values in the dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1169aeb9",
   "metadata": {},
   "source": [
    "After dealing with the missing values, we need to encode the categorical variables. <br>\n",
    "To do so we have created a mapping for each categorical variable, ordering each possible value of each categorical value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00d0e7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical = ['Smoking', 'Alcohol', 'Punctuality', 'Lying', 'Internet usage', 'Education', 'Gender', 'Hand', 'Only child', 'Home Town Type', 'Home Type']\n",
    "\n",
    "howToMap = {\n",
    "    'Smoking':          {'former smoker':   -1, 'never smoked':                     0,  'tried smoking':    1,  'current smoker':           3},\n",
    "    'Alcohol':          {'never':           0,  'social drinker':                   1,  'drink a lot':      2}, \n",
    "    'Punctuality':      {'early':           -1, 'on time':                          0,  'late':             1}, \n",
    "    'Lying':            {'never':           0,  'only to avoid hurting someone':    1,  'sometimes':        2,  'everytime it suits me':    3},\n",
    "    'Internet usage':   {'no time at all':  0,  'less than an hour a day':          1,  'few hours a day':  2,  'most of the day':          3},\n",
    "    'Education':        {\n",
    "        'currently a primary school pupil': 0, \n",
    "        'primary school':           1,\n",
    "        'secondary school':         2,\n",
    "        'college/bachelor degree':  3,\n",
    "        'masters degree':           4,\n",
    "        'doctorate degree':         5\n",
    "    },\n",
    "    'Gender':           {'female':              -1, 'male':             1}, \n",
    "    'Hand':             {'left':                -1, 'right':            1}, \n",
    "    'Only child':       {'no':                  -1, 'yes':              1}, \n",
    "    'Home Town Type':   {'village':             -1, 'city':             1}, \n",
    "    'Home Type' :       {'block of flats':      -1, 'house/bungalow':   1},\n",
    "}\n",
    "\n",
    "# Applying mapping to each column in the dataframe\n",
    "for colName, valueMapping in howToMap.items():\n",
    "    colSeries = data_fixed_df[colName].copy()\n",
    "\n",
    "    for valueToMap, substitution in valueMapping.items():\n",
    "        colSeries[colSeries == valueToMap] = substitution\n",
    "\n",
    "    data_fixed_df[colName] = colSeries.copy()\n",
    "\n",
    "    # data_fixed_df[colName] = data_fixed_df[colName].astype(float)\n",
    "\n",
    "data_fixed_df = data_fixed_df.astype(float)\n",
    "\n",
    "# if everything is correctly encoded, there should be no categorical variables\n",
    "if any(data_fixed_df.dtypes == 'object'):\n",
    "    raise ValueError(\"categorical variables in the dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755192f4",
   "metadata": {},
   "source": [
    "We can now randomly select $\\frac{2}{3}$ of the dataset to work with and visualize what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fb1692",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_df:pd.DataFrame\n",
    "\n",
    "X_df, _ = sklearn.model_selection.train_test_split(data_fixed_df[these_features], test_size=0.33, random_state=random_seed) \n",
    "# X_df = data_fixed_df[these_features].sample(frac=2/3, random_state=random_seed)\n",
    "X_df = X_df.sort_index()\n",
    "\n",
    "# if everything is correct we should only be dealing with floating point numbers\n",
    "print(\"Type of data in resulting dataframe: \", *np.unique(X_df.dtypes)) \n",
    "print('X_df:')\n",
    "display(X_df)\n",
    "\n",
    "desc = X_df.describe()\n",
    "display(desc) \n",
    "\n",
    "print(\"Standard deviation in descending order:\")\n",
    "variances = pd.DataFrame(desc.loc['std'].iloc[np.argsort(-desc.loc['std', :])], columns=['std']).T\n",
    "display(variances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a9271",
   "metadata": {},
   "source": [
    "For a better understanding of the mean and the standard deviation values we can create a scatter plot of both the mean and standard deviation values of the features. <br>\n",
    "We can also distinguish between the unmodified and encoded features by coloring them differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e83877",
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = np.array([cm.tab10(int(x in categorical)) for x in X_df.columns])\n",
    "\n",
    "legend_encoded_handle = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cm.tab10(i), markersize=10, label=label) for i, label in enumerate(['unmodified', 'encoded'])]\n",
    "\n",
    "# mask = [x in howToMap.keys() for x in desc.columns]\n",
    "# mask_not = [not x for x in mask]\n",
    "\n",
    "# idx_encoded = [i for i, value in enumerate(X_df.columns) if value in howToMap.keys()]\n",
    "# idx_original = [i for i, value in enumerate(X_df.columns) if value not in howToMap.keys()]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=desc.loc['mean', :], \n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(\"Mean of the features\")\n",
    "plt.xlabel(\"i-th column\")\n",
    "plt.ylabel(\"Mean\")\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=desc.loc['std', :], \n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(\"Standard deviation of the features\")\n",
    "plt.xlabel(\"i-th column\")\n",
    "plt.ylabel(\"Standard Deviation of the features\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4058cb9-19fc-406c-8380-7e8053b43649",
   "metadata": {},
   "source": [
    "## Exercise 2. Analyzing the Variance and the PCs\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. create two new dataframes from _X_df_ applying a StandardScaler and a MinMaxscaler. Call these new dataframes as _Xstd_df_ and _Xmm_df_, respectively;\n",
    "2. compute the variance of all the features in _X_df_, _Xstd_df_, and _Xmm_df_ and **comment the results**;\n",
    "3. compute all the $n$ Principal Components (PCs) for each dataset _X_df_, _Xstd_df_, and _Xmm_df_. Then, visualize the curves of the cumulative explained variances and **comment the results**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2db9e9",
   "metadata": {},
   "source": [
    "### 1 - Apply StandardScaler and MinMaxScaler\n",
    "Since we are dealing with all numerical features, we can now create two new dataframes from X_df applying a StandardScaler and a MinMaxScaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0f2237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mm_ranges = (0,1)\n",
    "Xstd_df = pd.DataFrame(StandardScaler().fit_transform(X_df), index=X_df.index, columns=X_df.columns)\n",
    "Xmm_df = pd.DataFrame(MinMaxScaler(mm_ranges).fit_transform(X_df), index=X_df.index, columns=X_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4de118",
   "metadata": {},
   "source": [
    "### 2 - Compute variance\n",
    "We can now compute the variance of all the features in X_df, Xstd_df, and Xmm_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662fe494",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variances(X:pd.DataFrame, title:str):    \n",
    "    fig = plt.figure(figsize=(10,7))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    ax0 = fig.add_subplot(211)\n",
    "    ax1 = fig.add_subplot(212)\n",
    "\n",
    "    ax0.scatter(\n",
    "        x=np.arange(X.shape[1]),\n",
    "        y=X.loc['std', :], \n",
    "        c=colors,\n",
    "    )\n",
    "    ax0.legend(handles=legend_encoded_handle)\n",
    "    ax0.set_ylabel('Variance')\n",
    "    ax0.grid()\n",
    "\n",
    "    ax1.scatter(\n",
    "        x=np.arange(X.shape[1]),\n",
    "        y=X.loc['mean', :], \n",
    "        c=colors,\n",
    "    )\n",
    "    ax1.legend(handles=legend_encoded_handle)\n",
    "    ax1.set_xlabel('Features')\n",
    "    ax1.set_ylabel('Mean')\n",
    "    ax1.grid()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "X_desc = X_df.describe()\n",
    "Xmm_desc = Xmm_df.describe()\n",
    "Xstd_desc = Xstd_df.describe()\n",
    "\n",
    "plot_variances(X_desc, \"X_df\")\n",
    "plot_variances(Xstd_desc, \"Xstd_df\")\n",
    "plot_variances(Xmm_desc, \"Xmm_df\")\n",
    "\n",
    "rescaling_factors = X_desc.loc['std', :] / Xmm_desc.loc['std', :]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    np.arange(X_df.shape[1]),\n",
    "    rescaling_factors,\n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(\"Scaling factor in MinMax scaling\")\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Scaling Factor')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762be74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = X_df.max() - X_df.min()\n",
    "\n",
    "min_mode = X_desc.loc['min', :].mode()[0]\n",
    "max_mode = X_desc.loc['max', :].mode()[0]\n",
    "\n",
    "different_delimiters = X_desc.loc[['min', 'max'], (X_desc.loc['min', :] != min_mode) | (X_desc.loc['max', :] != max_mode)].T\n",
    "different_delimiters['range'] = diff\n",
    "\n",
    "print(f\"Features with different ranges: {different_delimiters[different_delimiters['range'] != (max_mode - min_mode)].index.tolist()}\")\n",
    "display(different_delimiters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83793299",
   "metadata": {},
   "source": [
    "In the original dataframe **X_df** in the first figure, more or less all variances have the same order of magnitude, so when we will compute the PCA, our results won't be influenced much by the values of the features. But it's not guaranteed in general since a feature with high variance will absorb all information, thus being labeled as the feature with the highest explained variance ratio.\n",
    "\n",
    "When we apply standardization, this problem is absent, so all the features contributes equally to the PCA analysis regardless of their numerical ranges.\n",
    "\n",
    "And we do in fact get that $\\forall col \\in Xstd\\_df.columns$:\n",
    "\\begin{align*}\n",
    "Var(Xstd\\_df[col]) &\\approx 1\\\\\n",
    "E(Xstd\\_df[col]) &\\approx 0\n",
    "\\end{align*}\n",
    "Which is to be expected since the standardization process removes the mean and scales the variance to 1. Thus the data contained in the original dataframe have been standardized. <br>\n",
    "\n",
    "In the last dataframe **Xmm_df** the normalization let us remove the problem of features with large numerical ranges that may end up being more important than features with smaller numerical ranges. To do so, the MinMaxScaler scales all features to the range $[0,1]$. Such that all the variances of the features will be comparable. We can write the action of the MinMaxScaler as: $Xmm\\_df[col] = \\frac{X\\_df[col] - min(X\\_df[col])}{max(X\\_df[col]) - min(X\\_df[col])}$ <br> \n",
    "Such that $\\forall col \\in Xmm\\_df.columns$:\n",
    "\\begin{align*}\n",
    "max(Xmm\\_df[col]) = 1\\\\\n",
    "min(Xmm\\_df[col]) = 0\\\\\n",
    "\\end{align*}\n",
    "So $Var(Xmm\\_df[col])$ will be the rescaled version of the original variance, but we don't have much control on $E(Xmm\\_df[col])$ and for this analysis it's not relevant.\n",
    "\n",
    "In this particular dataset almost all features have similar numerical ranges so the result of this scaling technique maintains the distribution of variance of the original data, but there are some features who does not have a minimum of *1* and a maximum of *5*. Almost all of them are coming from how we encoded those values, the only two exceptions are *\"Fun with friends\"* wich is not encoded and has a minimum of *2*, and *\"Smoking\"*, an encoded features, that even if it has diffrent delimiters, the range that it lives in is the same as the usual case of *4*.\n",
    "\n",
    "In fact, observing the last figure titled *\"Scaling factor in MinMax scaling\"*, we can observe how having different ranges brings different scaling coefficients. And these anomalous columns are the only ones that change their relative variances between the variances in *X\\_df* and *Xmm\\_df*.\n",
    "\n",
    "So we can perform an additional rescaling before applying normalization again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c64104",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled_df = X_df*(rescaling_factors.mode()[0]/rescaling_factors)\n",
    "\n",
    "Xmm_df = pd.DataFrame(MinMaxScaler(mm_ranges).fit_transform(X_scaled_df), index=X_df.index, columns=X_df.columns)\n",
    "\n",
    "Xscaled_desc = X_scaled_df.describe()\n",
    "Xmm_desc = Xmm_df.describe()\n",
    "\n",
    "rescaling_factors_after = Xscaled_desc.loc['std', :] / Xmm_desc.loc['std', :]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(\n",
    "    np.arange(X_df.shape[1]),\n",
    "    rescaling_factors_after,\n",
    "    c=colors,\n",
    ")\n",
    "plt.legend(handles=legend_encoded_handle)\n",
    "plt.title(\"Scaling factor in MinMax scaling\")\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Scaling Factor')\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "fig = plt.figure(figsize=(10,7))\n",
    "fig.suptitle(\"Distribution of variance\")\n",
    "\n",
    "ax0 = fig.add_subplot(211)\n",
    "ax1 = fig.add_subplot(212)\n",
    "\n",
    "ax0.set_title(\"Xscaled_df\")\n",
    "ax0.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=Xscaled_desc.loc['std', :], \n",
    "    c=colors,\n",
    ")\n",
    "ax0.legend(handles=legend_encoded_handle)\n",
    "ax0.set_ylabel('Variance')\n",
    "ax0.grid()\n",
    "\n",
    "ax1.set_title(\"Xmm_df\")\n",
    "ax1.scatter(\n",
    "    x=np.arange(X_df.shape[1]),\n",
    "    y=Xmm_desc.loc['std', :], \n",
    "    c=colors,\n",
    ")\n",
    "ax1.legend(handles=legend_encoded_handle)\n",
    "ax1.set_xlabel('Features')\n",
    "ax1.set_ylabel('Variance')\n",
    "ax1.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d4b2a8",
   "metadata": {},
   "source": [
    "Now the scaling factors are the same regardless of the feature values, so features are scaled in the range *[0, 1]* and we were able to maintain the behavior of the original data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563c45e",
   "metadata": {},
   "source": [
    "### 3 - Compute Principal Components (PCA)\n",
    "We can now compute the principal components (PCs) for the standardized and normalized data and for the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "181358a6-6bb0-4f22-b395-34e3757fa16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_x = PCA(random_state=random_seed)\n",
    "pca_x_std = PCA(random_state=random_seed)\n",
    "pca_x_mm = PCA(random_state=random_seed)\n",
    "\n",
    "pca_x.fit(X_df)\n",
    "pca_x_std.fit(Xstd_df)\n",
    "pca_x_mm.fit(Xmm_df)\n",
    "\n",
    "Y_x = pca_x.transform(X_df)\n",
    "Y_x_std = pca_x_std.transform(Xstd_df)\n",
    "Y_x_mm = pca_x_mm.transform(Xmm_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d63e6db",
   "metadata": {},
   "source": [
    "We can now visualize the explained variance ratio for each dataset using the function `plot_explained_variance_ratio`, to better understand\n",
    "which feature are the most important features as regards the variance explained. <br>\n",
    "We can also plot the $0.33$ of cumulative explained variance to approximately see the number of components needed to explain approximately $33\\%$ of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641f95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_explained_variance_ratio(pca:PCA, title:str):    \n",
    "    plt.figure()\n",
    "    plt.plot(np.arange(pca.n_features_in_), np.cumsum(pca.explained_variance_ratio_))\n",
    "    plt.axhline(0.33, color='red', linestyle='--', label='y = 0.33')\n",
    "    plt.title(title)\n",
    "    plt.ylim([0, 1.1])\n",
    "    plt.xticks(ticks=np.arange(0, pca.n_features_in_, 10), \n",
    "            labels=[f'PC{i + 1}' for i in range(0, pca.n_features_in_, 10)])\n",
    "    plt.xlabel('Principal components')\n",
    "    plt.ylabel('Cumulative explained variance')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "plot_explained_variance_ratio(pca_x, 'Responses')\n",
    "plot_explained_variance_ratio(pca_x_std, 'Responses with standardization')\n",
    "plot_explained_variance_ratio(pca_x_mm, 'Responses with normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc230ffa",
   "metadata": {},
   "source": [
    "As highlighted in the previous point, the plot of cumulative explained variance from **X_df** and from **Xmm_df** are similar because the normalization had little effect on the distribution of total variance, but for **Xstd_df** the are some differences, for example with *11* principal components it does not quite reach *40%* of explained variance, while the other two dataframes does reach it, this is due to the fact that not having standardized/normalized the data will lead possibly lead to a feature having a higher importance than the others mainly because the variance of that feature is particularly high.\n",
    "\n",
    "An important observation is that in all cases the first principal component is more important than the others, hence why the graph does not starts at height close to *0*. And as we consider more *PC* their importance decrease so this is why the slope of the lines decreases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c1771a-09b2-41bc-bc4c-7e3f6f530021",
   "metadata": {},
   "source": [
    "## Exercise 3. Dimensionality Reduction and PC Interpretation\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two dataframes _Xstd_df_, and _Xmm_df_, compute a new PCA for performing a dimensionality reduction with respect to $m$ dimensions. The value of $m$ must be $$m = \\min\\{m', 5\\}\\,,$$ where $m'$ is the value required for obtaining $33\\%$ of the total variance.\n",
    "2. For both the cases, visualize all the PCs and give a name/interpretation to them. **Comment and motivate your interpretations**. If possible, **compare the differences among the results obtained** for _Xstd_df_ and _Xmm_df_.\n",
    "3. Perform the score graph for both the cases (_std_ and _mm_). If $m>3$, plot the score graph with respect to the first 3 PCs. All the **plots must show the names of the PCs on the axes** for better understanding the results.\n",
    "4. **Optional:** plot more score graphs, coloring the dots with respect to any label in the list _labels_ that you believe can be interesting. **Comment and analyze this optional plots**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0bffaf",
   "metadata": {},
   "source": [
    "### 1 - PERFORMING THE PCA\n",
    "We can now perform the PCA for both the standardized and normalized dataframes, either selecting the first $5$ components or a number of components that would explain at least $33\\%$ of the total variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6fe9bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_point_3_1(X_df:pd.DataFrame, pca:PCA):\n",
    "    a = np.cumsum(pca.explained_variance_ratio_)\n",
    "    m_prime = a[a < 0.33].shape[0] + 1\n",
    "    m = min(m_prime, 5)\n",
    "\n",
    "    pca_m = PCA(n_components=m, random_state=random_seed)\n",
    "\n",
    "    Y_m_df = pd.DataFrame(pca_m.fit_transform(X_df), columns=[f\"PC{i+1}\" for i in range(m)])\n",
    "\n",
    "    return pca_m, Y_m_df\n",
    "\n",
    "pca_x_std_m, Ystd_m_df = perform_point_3_1(Xstd_df, pca_x_std)\n",
    "pca_x_mm_m, Ymm_m_df = perform_point_3_1(Xmm_df, pca_x_mm)\n",
    "\n",
    "for i in range(min(pca_x_std_m.n_components_, pca_x_mm_m.n_components_)):\n",
    "    idx = np.argmax(pca_x_std_m.components_[i, :])\n",
    "    direction = np.sign(pca_x_std_m.components_[i, idx] * pca_x_mm_m.components_[i, idx])\n",
    "\n",
    "    direction = 1 if abs(direction) < 0.5 else direction\n",
    "    pca_x_mm_m.components_[i,:] *= direction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f0de08",
   "metadata": {},
   "source": [
    "The *for* in the last lines is to make sure that at least the first *PCs* will have the same verse.\n",
    "\n",
    "Of course this does not change the result of what the *PCA* picks up, only how to interpret the *PC* that are inverted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a01cd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# cat = labels[4]\n",
    "# Y = Ystd_m_df.copy()\n",
    "# Y[cat] = data_df.loc[X_df.index, cat].values\n",
    "# _ = sns.pairplot(Y, hue=cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612e6034",
   "metadata": {},
   "source": [
    "### 2 - VISUALIZE THE PCs\n",
    "we can now visualize the principal components (PCs) for both the standardized and normalized dataframes, and try to interpret them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82582e7e",
   "metadata": {},
   "source": [
    "#### SEPARATE PLOTS\n",
    "Firstly we can plot them separately to better understand the relationships between the PCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db46e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_index_categories(cmap_, cols, variables_by_type, these_types):\n",
    "    norm = plt.Normalize(0, len(these_types))\n",
    "    colors = []\n",
    "    for col in cols:\n",
    "        for i, t in enumerate(these_types):\n",
    "            var_list = variables_by_type[t]\n",
    "\n",
    "            if col in var_list:\n",
    "                colors.append(cmap_(norm(i)))\n",
    "                break\n",
    "    return colors\n",
    "\n",
    "def create_custom_legend(cmap_, these_types): \n",
    "    norm = plt.Normalize(0, len(these_types)) \n",
    "    legend_handles = [] \n",
    "    for i, t in enumerate(these_types): \n",
    "        legend_handles.append(plt.Line2D([0], [0], color=cmap_(norm(i)), lw=4, label=t))\n",
    "    \n",
    "    return legend_handles\n",
    "\n",
    "def plot_principal_components(pca:PCA, columns, title:str, pc_names:list[str], cmap_=cm.gist_ncar):\n",
    "    pcmax = pca.components_.max()*1.1\n",
    "    pcmin = pca.components_.min()*1.1\n",
    "\n",
    "    eps = np.sqrt(1/pca.n_features_in_)\n",
    "\n",
    "    \n",
    "    for i in range(pca.n_components_):\n",
    "        plt.figure(figsize=(10,15))\n",
    "\n",
    "        idx = np.argsort(pca.components_[i, :])\n",
    "\n",
    "        # plt.barh(np.arange(0, pca_x.n_features_in_, step) - bar_height_std/2, pca_x.components_[i, :][idx][::step], height=bar_height_mm, label='Normal')\n",
    "        plt.barh(np.arange(pca.n_features_in_), pca.components_[i, :][idx], color=get_index_categories(cmap_, columns[idx], variables_by_type, these_types))\n",
    "        plt.axvline(x= eps, color='red', linestyle='--', linewidth=2)\n",
    "        plt.axvline(x=-eps, color='red', linestyle='--', linewidth=2)\n",
    "\n",
    "        plt.yticks(ticks=np.arange(pca.n_features_in_), labels=columns[idx].tolist()) \n",
    "        plt.xlim((pcmin, pcmax))\n",
    "        plt.title(f'{title} responses, PC{i+1}: \\'{pc_names[i]}\\' with {100*pca.explained_variance_ratio_[i]:.2f}% of total variance')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "\n",
    "        legend_handles = create_custom_legend(cmap_, these_types) \n",
    "        plt.legend(handles=legend_handles, title=\"Feature Types\")\n",
    "        plt.show()\n",
    "\n",
    "        # display(f\"High values:{columns[pca.components_[i,:] > eps].tolist()}\")\n",
    "        # display(f\"Low values:{columns[pca.components_[i,:] < -eps].tolist()}\")\n",
    "\n",
    "pc_names_std = ['Cultured/Artistic', 'Delicate', 'Dynamic/Adaptable', 'Rebellious', 'Tech Oriented']\n",
    "pc_names_mm = ['Cultured/Artistic', 'Delicate', 'Dynamic/Adaptable', 'Social/Well-connected', 'Independent']\n",
    "\n",
    "plot_principal_components(pca_x_std_m, Xstd_df.columns, \"Standardized\", pc_names_std)\n",
    "plot_principal_components(pca_x_mm_m, Xmm_df.columns, \"Normalized\", pc_names_mm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fd32cf",
   "metadata": {},
   "source": [
    "Standardized dataframe:\n",
    "- PC1: this group of people is a mix of artistic and musical individuals who value culture and personal growth.\n",
    "- PC2: these individuals are more frightened by typical phobias and like to take care of themselves whether being it dressing or eating.\n",
    "- PC3: this type of people like fast-paced environments and enjoy making new connections, they are less frightened by common fears.\n",
    "- PC4: here independence is key, we can identify people who dislike following rules and love going to the bar to drink and socialize, \n",
    "\tpreferring a fast life rather.\n",
    "- PC5: individuals more oriented to technological fields who avoid politics and economy in discussions.\n",
    "\n",
    "Normalized dataframe:\n",
    "- PC1: we identify people more oriented to culture and artistic expressions.\n",
    "- PC2: people who are affected by common phobias and likes to take care of themselves.\n",
    "- PC3: these people like dynamic environments and prefer active and engaging activities and present a strong personality.\n",
    "- PC4: individuals that keeps up with current events and enjoy social activities but dislike STEM fields.\n",
    "- PC5: people who prefer breaking norms and rules, they prefer living the moment so not to waste any second of their life, disliking politics and showing a strong personality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852d9a14",
   "metadata": {},
   "source": [
    "#### 2.B COMPARING THE DIFFERENCES\n",
    "Now, we can compare the difference between the results of the two previously shown analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d36f34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_principal_components_comparisons(pca_x_std_m:PCA, pca_x_mm_m:PCA, columns:list[str]):\n",
    "    bar_space = 0.4\n",
    "    bar_height = 0.2\n",
    "\n",
    "    eps = np.sqrt(1/pca_x_std_m.n_features_in_)\n",
    "\n",
    "    m = min(pca_x_std_m.n_components_, pca_x_mm_m.n_components_)\n",
    "\n",
    "    pcmax = max(pca_x_std_m.components_[:m, :].max(), pca_x_mm_m.components_[:m, :].max())\n",
    "    pcmax *= 1+np.sign(pcmax)*0.1\n",
    "    pcmin = min(pca_x_std_m.components_[:m, :].min(), pca_x_mm_m.components_[:m, :].min())\n",
    "    pcmin *= 1-np.sign(pcmin)*0.1\n",
    "\n",
    "    for i in range(m):\n",
    "        plt.figure(figsize=(10,15)) \n",
    "        idx = np.argsort(pca_x_std_m.components_[i, :])\n",
    "\n",
    "        plt.barh(np.arange(pca_x_std_m.n_features_in_) + bar_space/2, pca_x_std_m.components_[i, idx], height=bar_height, label='Standardized')\n",
    "        plt.barh(np.arange(pca_x_mm_m.n_features_in_)  - bar_space/2, pca_x_mm_m.components_[i, idx], height=bar_height, label='Normalized')\n",
    "        \n",
    "        plt.axvline(x= eps, color='red', linestyle='--', linewidth=2)\n",
    "        plt.axvline(x=-eps, color='red', linestyle='--', linewidth=2)\n",
    "        \n",
    "        plt.xlim((pcmin, pcmax))\n",
    "        \n",
    "        plt.yticks(ticks=np.arange(pca_x_std_m.n_features_in_), labels=columns[idx].tolist()) \n",
    "        plt.title(f'Responses PC{i+1}')\n",
    "        plt.grid()\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_principal_components_comparisons(pca_x_std_m, pca_x_mm_m, X_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651e4f93",
   "metadata": {},
   "source": [
    "We can see that in the first two principal components there are some differences, but both *Xstd_df* and *Xmm_df* capture the same group of peoples.\n",
    "\n",
    "Starting from the third component, their meaning between the analyses start to diverge. The *PC3* can still be connected to the same type of people since the features with the highest and lowest values are basically the same. But from *PC4* we can no longer say that they represent the same individuals, this problem is even more highlighted in *PC5*.\n",
    "\n",
    "This happens because of two motives:\n",
    "\n",
    "1. We used different scaling techniques, so the information carried by the features themselves are different\n",
    "1. As we inspect deeper into the PCs, their explained variance becomes lower and lower, for example *PC1* explain about *8%* of the total variance and *PC5* just about *3%* of total variance. So the information that they represent is less and they should contribute less in the decisions arose from this analysis. Therefore, we should consider using a larger number of components or use a different method that doesn't loose information as much.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc9a183",
   "metadata": {},
   "source": [
    "#### 3 - SCORE GRAPH\n",
    "We can evaluate and plot the score graph using both X_std and X_mm plotting it for the first 3 PCs and adding the name of the PCs on the axes for better understanding the results.<br> \n",
    "Moreover, we will also plot the score graph using the first two PCs in just two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7edb6193",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_graph2D(ax:plt.Axes, Y:np.ndarray, title:str, ax_names:list[str], colors, s:float=20, all_same_color=False,):\n",
    "    if all_same_color:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], color=colors, s=s)\n",
    "    else:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], c=colors, s=s)\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(ax_names[0])\n",
    "    ax.set_ylabel(ax_names[1])\n",
    "\n",
    "    # if mappable is not None:\n",
    "    #     fig.colorbar(mappable=mappable, ax=ax)\n",
    "\n",
    "    ax.grid()\n",
    "    return ax\n",
    "\n",
    "def score_graph3D(ax:plt.Axes, Y:np.ndarray, title:str, ax_names:list[str], colors, s:float=20, all_same_color=False):\n",
    "    if all_same_color:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], color=colors, s=s)\n",
    "    else:\n",
    "        ax.scatter(Y[:, 0], Y[:, 1], Y[:, 2], c=colors, s=s)\n",
    "        \n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(ax_names[0])\n",
    "    ax.set_ylabel(ax_names[1])\n",
    "    ax.set_zlabel(ax_names[2])\n",
    "    \n",
    "    # if mappable is not None:\n",
    "    #     fig.colorbar(mappable=mappable, ax=ax)\n",
    "    \n",
    "    ax.grid()\n",
    "\n",
    "    return ax\n",
    "\n",
    "def isolated_score_graph(Y:np.ndarray, title:str, ax_names:list[str], cat:str, colors, s:float, all_same_color=False, mappable:cm.ScalarMappable=None, handles=None, mode:str='2d'):\n",
    "    if mode not in ['2d', '3d']:\n",
    "        raise ValueError(\"'mode' must be '2d' or '3d'\")\n",
    "    \n",
    "    scoregraph = plt.figure()\n",
    "\n",
    "    if mode == '2d':\n",
    "        ax = scoregraph.add_subplot()\n",
    "        score_graph2D(ax, Y, title, ax_names, colors, s, all_same_color)\n",
    "    else:\n",
    "        ax = scoregraph.add_subplot(projection='3d')\n",
    "        score_graph3D(ax, Y, title, ax_names, colors, s, all_same_color)\n",
    "\n",
    "    if all_same_color == False:\n",
    "        if mappable is not None:\n",
    "            fig.colorbar(ax=ax, mappable=mappable, label=cat) \n",
    "        else:\n",
    "            scoregraph.legend(loc='center right', bbox_to_anchor=(1, 0.5), handles=handles, title=cat)\n",
    "            # scoregraph.legend(handles=handles)\n",
    "    return scoregraph, ax\n",
    "\n",
    "def create_colors(values:pd.Series, label:str, use_continous:bool=False, use_default_discrete_correlator:bool=True, valueLabelCorrelator:dict=None, cmap_continuous = cm.viridis, cmap_discrete = cm.rainbow):\n",
    "    if use_continous:\n",
    "        Q1 = values.quantile(0.25) \n",
    "        Q3 = values.quantile(0.75) \n",
    "        IQR = Q3 - Q1 \n",
    "\n",
    "        lower_bound = Q1 - 1.5 * IQR \n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        cmap_ = cmap_continuous\n",
    "        norm = plt.Normalize(vmin=max(lower_bound, np.min(values)), vmax=min(upper_bound, np.max(values))) \n",
    "\n",
    "        # norm = plt.Normalize(vmin=np.min(values), vmax=np.max(values)) \n",
    "        \n",
    "        mappable = cm.ScalarMappable(norm=norm, cmap=cmap_)\n",
    "        handles = None\n",
    "    else:\n",
    "        if use_default_discrete_correlator:\n",
    "            correlator = howToMap[label]\n",
    "        else:\n",
    "            correlator = valueLabelCorrelator\n",
    "            \n",
    "        cmap_ = cmap_discrete\n",
    "        norm = plt.Normalize(vmin=np.min(values), vmax=np.max(values)) \n",
    "        \n",
    "        mappable = None\n",
    "        handles = [Line2D([0], [0], marker='o', color='none', linestyle='None', markeredgewidth=0, markerfacecolor=cmap_(norm(val)), markersize=10, label=label) for label, val in correlator.items()]\n",
    "\n",
    "    return cmap_, norm, mappable, handles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ee500c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_1 = cm.tab10\n",
    "\n",
    "isolated_score_graph(Ystd_m_df.values, \"STANDARDIZED - SCORE GRAPH\", pc_names_std, '', cmap_1(1), s=20, all_same_color=True, mode='2d'); plt.show()\n",
    "isolated_score_graph(Ymm_m_df.values,  \"NORMALIZED - SCORE GRAPH\",   pc_names_mm,  '', cmap_1(0), s=20, all_same_color=True, mode='2d'); plt.show()\n",
    "isolated_score_graph(Ystd_m_df.values, \"STANDARDIZED - SCORE GRAPH\", pc_names_std, '', cmap_1(1), s=20, all_same_color=True, mode='3d'); plt.show()\n",
    "isolated_score_graph(Ymm_m_df.values,  \"NORMALIZED - SCORE GRAPH\",   pc_names_mm,  '', cmap_1(0), s=20, all_same_color=True, mode='3d'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd4a5d26",
   "metadata": {},
   "source": [
    "The score graphs obtained in both 2D and 3D are quite similar, in fact, we can recall that the first three principal components found lead to the same three profiles. However from the fourth one onwards the profile found start to diverge, so if we could plot 4D data, we could see more differences in the profiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe21958",
   "metadata": {},
   "source": [
    "#### 4 - MORE IN-DEPTH ANALYSIS\n",
    "We can perform a more in-depth analysis by plotting the score graph and coloring the labels accordingly to the previously found results. <br>\n",
    "Note: at the end of the analysis you will find some plots and some comments on said plots, as we will see only some plots and labeling yielded meaningful results, and as such, not to create confusion, we only plotted the meaningful results and respective plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48406cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_score_graphs_colored(Y:pd.DataFrame, title:str, cat:str, pc_names:list[str], colors, mappable=None, handles=None):\n",
    "    m = min(Y.shape[1], 3)\n",
    "\n",
    "    fig = plt.figure(figsize=(14,12))\n",
    "\n",
    "    fig.suptitle(title)\n",
    "    axes = fig.subplots(m, m)\n",
    "\n",
    "    for i in range(m):\n",
    "        for j in range(m):\n",
    "            if (i == m-1) | (j == 0):\n",
    "                xname = pc_names[j] if i == m-1 else ''\n",
    "                yname = '' if j else pc_names[i]\n",
    "            else:\n",
    "                xname =''\n",
    "                yname =''\n",
    "\n",
    "            score_graph2D(axes[i, j], Y=Y.values[:, [j,i]], title='', ax_names=[xname, yname], colors=colors, s=20, all_same_color=False)\n",
    "\n",
    "            if i < m-1:\n",
    "                plt.setp(axes[i,j].get_xticklabels(), visible=False)\n",
    "            if j > 0:\n",
    "                plt.setp(axes[i,j].get_yticklabels(), visible=False)\n",
    "\n",
    "    # score_graph2D(fig, axes[1], Y=Ystd_m_df.values[:, [z, y]], title=f\"PC{z} (hor.) vs PC{y} (ver.)\", ax_names=[pc_names_std[i] for i in [z, y]], colors=cmap_(norm(values)),  all_same_color=False)\n",
    "    # axes.append(score_graph2D(Y=Ymm_m_df.values,  title=f\"NORMALIZED - SCORE GRAPH, colored on {cat}\",       ax_names=pc_names_mm,  colors=cmap_(norm(values)),  all_same_color=False))\n",
    "    # axes.append(score_graph3D(Y=Ystd_m_df.values, title=f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", ax_names=pc_names_std, colors=cmap_(norm(values)),  all_same_color=False))\n",
    "    # axes.append(score_graph3D(Y=Ymm_m_df.values,  title=f\"NORMALIZED - SCORE GRAPH, colored on {cat}\",       ax_names=pc_names_mm,  colors=cmap_(norm(values)),  all_same_color=False))\n",
    "\n",
    "    if handles is not None:\n",
    "        # fig.legend(loc='center right', bbox_to_anchor=(1, 0.5), handles=handles)\n",
    "        fig.legend(handles=handles, title=cat)\n",
    "    else:\n",
    "        fig.colorbar(ax=axes, mappable=mappable, label=cat)\n",
    "\n",
    "    return fig\n",
    "\n",
    "# for i in [1]:\n",
    "for i in [1, 2, 4]:\n",
    "    cat = labels[i]\n",
    "    values = data_fixed_df.loc[X_df.index, cat]\n",
    "\n",
    "    cmap_, norm, mappable, handles = create_colors(values, labels[i], labels[i] not in categorical)\n",
    "\n",
    "    fig = plot_score_graphs_colored(Ystd_m_df, title=f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", cat=cat, pc_names=pc_names_std, colors=cmap_(norm(values)), mappable=mappable, handles=handles)\n",
    "    plt.show()\n",
    "\n",
    "    fig = plot_score_graphs_colored(Ymm_m_df, title=f\"NORMALIZED - SCORE GRAPH, colored on {cat}\", cat=cat, pc_names=pc_names_mm, colors=cmap_(norm(values)), mappable=mappable, handles=handles)\n",
    "    plt.show()\n",
    "\n",
    "    y = 0\n",
    "    x = 1\n",
    "    z = 2\n",
    "    \n",
    "    fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [x, y]], title=f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", ax_names=[pc_names_std[i] for i in [x, y]], cat=cat, colors=cmap_(norm(values)), s=30, all_same_color=False, mappable=mappable, handles=handles, mode='2d')\n",
    "    plt.show()\n",
    "    fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [z, y]], title=f\"STANDARDIZED - SCORE GRAPH, colored on {cat}\", ax_names=[pc_names_std[i] for i in [z, y]], cat=cat, colors=cmap_(norm(values)), s=30, all_same_color=False, mappable=mappable, handles=handles, mode='2d')\n",
    "    plt.show()\n",
    "    \n",
    "    fig, ax = isolated_score_graph(Y=Ymm_m_df.values[:, [x, y]], title=f\"NORMALIZED - SCORE GRAPH, colored on {cat}\", ax_names=[pc_names_mm[i] for i in [x, y]], cat=cat, colors=cmap_(norm(values)), s=30, all_same_color=False, mappable=mappable, handles=handles, mode='2d')\n",
    "    plt.show()\n",
    "    fig, ax = isolated_score_graph(Y=Ymm_m_df.values[:, [z, y]], title=f\"NORMALIZED - SCORE GRAPH, colored on {cat}\", ax_names=[pc_names_mm[i] for i in [z, y]], cat=cat, colors=cmap_(norm(values)), s=30, all_same_color=False, mappable=mappable, handles=handles, mode='2d')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a92ac9",
   "metadata": {},
   "source": [
    "The first thing we did is to divide the labels in *2*, the first group can be seen as continuous and it's composed of *'Age', 'Height', 'Weight', 'Number of siblings'* even if *'a sibling and a half'* does not exist, while the other is discrete and composed of the remaining *'Gender', 'Hand', 'Education', 'Only child', 'Home Town Type', 'Home Type'*.\n",
    "\n",
    "In the figures of $m\\times m$ score graphs, the ones on the main diagonal help to see if a *PC* is correlated to the value of the label. The ones off the main diagonal are specular, so theoretically only the upper (lower) triangular part is useful.\n",
    "\n",
    "$% The score graph based on *PC1* vs. *PC2* and *PC1* vs. *PC3* are added isolated from the rest to help with insights.$\n",
    "\n",
    "We need to highlight that for the non discrete labels we applied some outlier filtering based on inter-quartile ranges just to make more evident the coloring of most of the population. Otherwise an outlier would have ruined the coloring palette, for example in *'Height'* there is a single person tall *60*, and a similar problem occur in *'Weight' and 'Number of siblings'*.\n",
    "\n",
    "- If we color based on *'Age'*, we can observe that there isn't any correlation between label and *PC* value\n",
    "- If we instead color on *'Height'*, we can see how taller peoples tend to be a bit less oriented to art, but it's not heavily correlated, they tend to be less scared by usual phobias and they are more dynamic, probably because of sports like basketball which favors tall peoples.\n",
    "- For *'Weight'* instead, rounder people are less scared and tends to be more dynamic, this correlation exists maybe because taller people usually weight more than smaller ones. Regarding art, there's not much more that we can extract because all values of weight are spread out in the whole axis related to art.\n",
    "- When we color by *'Number of siblings'* we do not obtain any new information\n",
    "- More interesting is the coloring based on *'Gender'*, where we can more evidently see how females and men approach art, fears and changes. Of course the division is evident but not rigid since there will always be exceptions. But we can see how females tends to be more scared of common phobias and to prefer more stable and tranquil environments. Regarding art, females tend to prefer it more than men.\n",
    "- For *'Hand', 'Education', 'Only child', 'Home Town Type', 'Home Type'* there isn't any new information that can be derived."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c8f0b8-37c5-403a-93ad-8abefd96e3b2",
   "metadata": {},
   "source": [
    "## Exercise 4. $k$-Means\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), run the $k$-Means for clustering the data. In particular, **use the silohuette score for identify the best value for $k\\in\\{3, \\ldots, 10\\}$**.\n",
    "2. Plot the score graphs of exercise 3.3, adding the centroids of the cluster.\n",
    "3. Observing the centroids coordinates in the PC space, **give a name/interpretation to them**, exploiting the names you assigned to the PCs. **Comment and motivate your interpretations**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e04088",
   "metadata": {},
   "source": [
    "### 1 - RUNNING KMEANS\n",
    "We can run the $k$-Means for both datasets and for different values of $k$. We will use the silhouette score to evaluate the performance of the clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2e7be2-1756-4e44-9bcd-56a0fa7c78b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kmeans(Y)-> tuple[KMeans, pd.DataFrame]:\n",
    "    km_list = []\n",
    "\n",
    "    silcoeff_list = pd.DataFrame(index=range(3,11), columns=['silhouette'])\n",
    "\n",
    "    silcoeff_list.index.name = 'k'\n",
    "\n",
    "    for i in range(silcoeff_list.shape[0]):\n",
    "        km_list.append(KMeans(n_clusters=silcoeff_list.index[i], random_state=random_seed))\n",
    "        km = km_list[i]\n",
    "        km_list[i] = km.fit(Y)\n",
    "\n",
    "        silcoeff_list.iloc[i, :] = silhouette_score(Y, labels=km.fit_predict(Y))\n",
    "\n",
    "    i_best = np.argmax(silcoeff_list.iloc[:, 0])\n",
    "    k = silcoeff_list.index[i_best]\n",
    "    km = km_list[i_best]\n",
    "\n",
    "    print('****************** RESULTS OF THE SEARCH... ******************')\n",
    "    print(f'BEST SILHOUETTE SCORE: {silcoeff_list.iloc[i_best, 0]} --> k = {k}')\n",
    "    print('**************************************************************')\n",
    "\n",
    "    display(silcoeff_list.sort_values(by='silhouette', ascending=False))\n",
    "    return km, silcoeff_list\n",
    "\n",
    "print(\"Standardized\")\n",
    "km_std_best, results_std = run_kmeans(Ystd_m_df)\n",
    "print(\"Normalized\")\n",
    "km_mm_best, results_mm = run_kmeans(Ymm_m_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc88479a",
   "metadata": {},
   "source": [
    "We can plot the obtained silhouette scores to understand better the behavior as the number of clusters varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057b2a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_std.sort_index()\n",
    "results_mm.sort_index()\n",
    "\n",
    "fig, ax = plt.subplots(1,1, figsize=(7,7)) \n",
    "\n",
    "ax.plot(results_mm.index, results_mm, 'red')\n",
    "ax.scatter(results_mm.index, results_mm, color='red', marker='*', label='X_mm')\n",
    "\n",
    "ax.plot(results_std.index, results_std, 'blue')\n",
    "ax.scatter(results_std.index, results_std, color='blue', marker='x', label='X_std')\n",
    "\n",
    "ax.set_title('Comparing silhouette score obtained')\n",
    "ax.legend(loc='upper right')\n",
    "ax.set_xticks(list(set(results_std.index).union(set(results_mm.index))))\n",
    "ax.set_ylabel('Silhouette score computed')\n",
    "ax.set_xlabel('Number of clusters')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05cc6b04",
   "metadata": {},
   "source": [
    "As we can see for both X_std and X_mm the best silhouette score is obtained using just $3$ clusters. Moreover, even if for the same number of clusters the two scores are quite similar, the X_mm score is consistently better than the X_std score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bd0258",
   "metadata": {},
   "source": [
    "### 2 - PLOTTING WITH CENTROIDS\n",
    "We can plot the previously obtained results at point $3.3$ adding the centroids found by running the $k$-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483dda1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap_3 = cm.tab10\n",
    "s = 20\n",
    "\n",
    "cluster_assignment_std = km_std_best.predict(Ystd_m_df)\n",
    "cluster_assignment_mm = km_mm_best.predict(Ymm_m_df)\n",
    "norm_clusters_std = plt.Normalize(vmin=np.min(cluster_assignment_std), vmax=np.max(cluster_assignment_std)) \n",
    "norm_clusters_mm = plt.Normalize(vmin=np.min(cluster_assignment_mm), vmax=np.max(cluster_assignment_mm)) \n",
    "\n",
    "# display(create_custom_legend(cmap_3, np.unique(cluster_assignment_std)))\n",
    "\n",
    "handles_std = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap_3(norm_clusters_std(c)), markersize=10, label=c) for c in np.unique(cluster_assignment_std)]\n",
    "handles_mm = [Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap_3(norm_clusters_mm(c)), markersize=10, label=c) for c in np.unique(cluster_assignment_mm)]\n",
    "\n",
    "# plot_score_graphs_colored(Ystd_m_df, f\"STANDARDIZED - SCORE GRAPH, colored on cluster assignment\", 'Cluster', pc_names_std, cmap_3(norm_clusters_std(cluster_assignment_std)), handles=handles_std)\n",
    "# plt.show()\n",
    "\n",
    "x,y = 1,0\n",
    "fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [x, y]], title=\"STANDARDIZED - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_std[i] for i in [x, y]], cat='Cluster', colors=cmap_3(norm_clusters_std(cluster_assignment_std)), s=s, all_same_color=False, mappable=None, handles=handles_std, mode='2d')\n",
    "ax.scatter(km_std_best.cluster_centers_[:, x], km_std_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "x,y = 2,0\n",
    "fig, ax = isolated_score_graph(Y=Ystd_m_df.values[:, [x, y]], title=\"STANDARDIZED - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_std[i] for i in [x, y]], cat='Cluster', colors=cmap_3(norm_clusters_std(cluster_assignment_std)), s=s, all_same_color=False, mappable=None, handles=handles_std, mode='2d')\n",
    "ax.scatter(km_std_best.cluster_centers_[:, x], km_std_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "x,y = 1,0\n",
    "fig, ax = isolated_score_graph(Y=Ymm_m_df.values[:, [x, y]], title=\"NORMALIZED - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_mm[i] for i in [x, y]], cat='Cluster', colors=cmap_3(norm_clusters_mm(cluster_assignment_mm)), s=s, all_same_color=False, mappable=None, handles=handles_mm, mode='2d')\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, x], km_mm_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "x,y = 2,0\n",
    "fig, ax = isolated_score_graph(Y=Ymm_m_df.values[:, [x, y]], title=\"NORMALIZED - SCORE GRAPH, colored on cluster assignment\", ax_names=[pc_names_mm[i] for i in [x, y]], cat='Cluster', colors=cmap_3(norm_clusters_mm(cluster_assignment_mm)), s=s, all_same_color=False, mappable=None, handles=handles_mm, mode='2d')\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, x], km_mm_best.cluster_centers_[:, y], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = isolated_score_graph(Ystd_m_df.values, 'STANDARDIZED - SCORE GRAPH, colored on cluster assignment', pc_names_std, 'Cluster', cmap_3(norm_clusters_std(cluster_assignment_std)), s=s, all_same_color=False, mappable=None, handles=handles_std, mode='3d')\n",
    "ax.scatter(km_std_best.cluster_centers_[:, 0], km_std_best.cluster_centers_[:, 1], km_std_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n",
    "\n",
    "fig, ax = isolated_score_graph(Ymm_m_df.values, 'NORMALIZED - SCORE GRAPH, colored based on cluster assignment', pc_names_mm, 'Cluster', cmap_3(norm_clusters_mm(cluster_assignment_mm)), s=s, all_same_color=False, mappable=None, handles=handles_mm, mode='3d')\n",
    "ax.scatter(km_mm_best.cluster_centers_[:, 0], km_mm_best.cluster_centers_[:, 1], km_mm_best.cluster_centers_[:, 2], c='black', marker='*')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a2bc49",
   "metadata": {},
   "source": [
    "### 3 - CENTROID MEANING \n",
    "Based on previous analysis we can try to infer the name and the meaning of the centroids found by the $k$-Means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb169142",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxs_std_m = Ystd_m_df.max(axis=0) \n",
    "mins_std_m = Ystd_m_df.min(axis=0) \n",
    "\n",
    "maxs_mm_m = Ymm_m_df.max(axis=0) \n",
    "mins_mm_m = Ymm_m_df.min(axis=0) \n",
    "\n",
    "k_std = km_std_best.cluster_centers_.shape[0]\n",
    "k_mm = km_mm_best.cluster_centers_.shape[0]\n",
    "\n",
    "fig_mm, ax_mm = plt.subplots(1, k_mm, figsize=(15, 7))\n",
    "\n",
    "for ii in range(k_mm):\n",
    "    ax_mm[ii].barh(np.arange(km_mm_best.cluster_centers_.shape[1]), maxs_mm_m, color='blue', alpha=0.15)\n",
    "    ax_mm[ii].barh(np.arange(km_mm_best.cluster_centers_.shape[1]), mins_mm_m, color='blue', alpha=0.15)\n",
    "    ax_mm[ii].barh(np.arange(km_mm_best.cluster_centers_.shape[1]), km_mm_best.cluster_centers_[ii, :])\n",
    "    \n",
    "    ax_mm[ii].set_yticks(ticks=np.arange(km_mm_best.cluster_centers_.shape[1]))\n",
    "    if ii == 0:\n",
    "        ax_mm[ii].set_yticklabels(labels=[f\"{name} (PC{i+1})\" for i, name in enumerate(pc_names_mm)], rotation=60)\n",
    "    else:\n",
    "        ax_mm[ii].set_yticklabels(labels=[])\n",
    "\n",
    "    \n",
    "    ax_mm[ii].grid(visible=True, which='both')\n",
    "    ax_mm[ii].set_title(f'Normalized - CENTROID {ii+1}')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "fig_std, ax_std = plt.subplots(1, k_std, figsize=(15, 7))\n",
    "for ii in range(k_std):\n",
    "    ax_std[ii].barh(np.arange(km_std_best.cluster_centers_.shape[1]), maxs_std_m, color='blue', alpha=0.15)\n",
    "    ax_std[ii].barh(np.arange(km_std_best.cluster_centers_.shape[1]), mins_std_m, color='blue', alpha=0.15)\n",
    "    ax_std[ii].barh(np.arange(km_std_best.cluster_centers_.shape[1]), km_std_best.cluster_centers_[ii, :])\n",
    "\n",
    "    ax_std[ii].set_yticks(ticks=np.arange(km_std_best.cluster_centers_.shape[1]))\n",
    "    if ii == 0:\n",
    "        ax_std[ii].set_yticklabels(labels=[f\"{name} (PC{i+1})\" for i, name in enumerate(pc_names_std)], rotation=60)\n",
    "    else:\n",
    "        ax_std[ii].set_yticklabels(labels=[])\n",
    "\n",
    "    ax_std[ii].grid(visible=True, which='both')\n",
    "    ax_std[ii].set_title(f'Standardized - CENTROID {ii+1}')\n",
    "\n",
    "plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e1f3d9",
   "metadata": {},
   "source": [
    "Regarding the clustering performed on **Xmm_df** in PC space we can observe that since the points are basically a cloud, the centers will be equidistant from each other, and in fact from the first two main *PC* this happens.\n",
    "\n",
    "So the archetypes of customers that we can identify are the three most diverse from each other:\n",
    "1. Likes art and it's a bit more introvert (Art Enthusiast)\n",
    "1. Does not like art and tends to be introvert (Calm Rationalist)\n",
    "1. Indifferent to art and it's extrovert (Social Realist)\n",
    "\n",
    "The same thing happens for **Xstd_df**, meaning we can find the same archetype of customers. The only difference is that the numerical value associated to these centroids are different, but still maintain similar proportion with respect to the maximum and minimum value of that feature in PC space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5313a218",
   "metadata": {},
   "outputs": [],
   "source": [
    "centroids_names = ['Art Enjoyer', 'Calm Rationalist', 'Social Realist']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb169142",
   "metadata": {},
   "source": [
    "## Exercise 5. Cluster Evaluations\n",
    "\n",
    "In the cells below, do the following operations:\n",
    "1. For each one of the two datasets (_std_ and _mm_), perform an **external evaluation** of the clustering obtained at exercise 4.1 with respect to one or more labels in the list _labels_. **Comment the results, comparing the evaluation with the interpretation you gave at exercise 4.3**. \n",
    "2. For each one of the two datasets (_std_ and _mm_), perform an **internal evaluation** of each cluster, with respect to the silohuette score. **Comment the results**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecafa1a3",
   "metadata": {},
   "source": [
    "### 1 - External Evaluation for X_std_ and  X_mm_ datasets\n",
    "We can perform an external evaluation of the two datasets with respect to the clustering results previously obtained.<br>\n",
    "Moreover, we can also compare the results with the interpretation previously given at point $4.3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780fd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def external_evaluation(cluster_assignment:np.ndarray, label_series:pd.Series, title:str, label:str):\n",
    "    distribution = pd.crosstab(cluster_assignment, label_series)\n",
    "\n",
    "    distribution.index.name = 'Cluster'\n",
    "    distribution.columns.name = label\n",
    "\n",
    "    # distribution.plot(ax=ax, kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "\n",
    "    total_counts = distribution.sum(axis=1)\n",
    "\n",
    "    cmap_, norm, mappable, handles = create_colors(label_series, label, label not in categorical, cmap_continuous=cm.rainbow)\n",
    "\n",
    "    percent = 0.8\n",
    "    width = percent/distribution.shape[1]\n",
    "    multiplier = -distribution.shape[1]/2.0 + width/percent\n",
    "\n",
    "    x = np.arange(distribution.shape[0])\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "    fig.suptitle(title)\n",
    "\n",
    "    for colName in distribution.columns:\n",
    "        offset = width*multiplier\n",
    "\n",
    "        ax.bar(\n",
    "            x + offset, \n",
    "            height=distribution[colName]/total_counts, \n",
    "            width=width,\n",
    "            color = cmap_(norm(colName))\n",
    "        )\n",
    "        multiplier += 1\n",
    "\n",
    "    if mappable is not None:\n",
    "        fig.colorbar(ax=ax, mappable=mappable, label=label)\n",
    "    else:\n",
    "        fig.legend(handles=handles, loc='center right', title=label)\n",
    "\n",
    "    ax.set_ylabel('Frequency')\n",
    "    ax.set_title(f'Distribution of {label} in clusters')\n",
    "    ax.set_xticks(x, centroids_names)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# for i in [7, 8, 9]:\n",
    "for i in [1,2,4]:\n",
    "    external_evaluation(cluster_assignment_std, data_fixed_df.loc[Xstd_df.index, labels[i]], title=\"Standardized dataset\", label=labels[i])\n",
    "    external_evaluation(cluster_assignment_mm, data_fixed_df.loc[Xmm_df.index, labels[i]], title=\"Normalized dataset\", label=labels[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a486d98",
   "metadata": {},
   "source": [
    "The figures are structured to highlight with respect to each cluster, denoted by the name of the associated centroid, the distribution of labels normalized by the total number of customers per cluster.\n",
    "\n",
    "To know if a label is characterizing of a centroid, we need to observe if the distribution of the values of a label between the clusters is notably different.\n",
    "\n",
    "For example, with *'Gender'* we can clearly see how in **some** clusters there's more males or females, so this label **is** important, the counter example is *'Hand'*, where even if in a cluster almost all of them are right-handed, all of the clusters have the **same** distribution of labels, so it's **not** meaningful.\n",
    "\n",
    "Explained this, we can continue with the results.\n",
    "\n",
    "Based on the data, we can conclude that:\n",
    "- Someone that likes art and it's an introvert, tends to have an height in the lower range of approx 165 - 175 cm, and to have a lower weight, in most of the cases it's a female but it's not a marked division. So we can conclude that this group is composed of **minute individuals** independent form the gender. \n",
    "- Peoples that don't quite like art and are more introvert tend to have an height in the range of approx 165 - 190 cm, and the weight is more on the central side, from 65-90 kg. This much variation is because part of this group are women and they occupy in general the lower part of these numerical ranges, while the men present in this group increase the frequency associated to the high numerical values of the label. So we can conclude that this group encapsulate **tranquil women** and **tall men**. \n",
    "- Individuals more social and realistic tends to have an height more evenly distributed between the whole numerical values of the label, with a peak at about 165 cm. For the weight we can see how it's more concentrated on the lower numerical side, from even less than 50 to about 70 kg. Lastly in most of the cases these peoples are female. We can conclude that this group is composed of **thin men** and most **women**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3425815e",
   "metadata": {},
   "source": [
    "### 2 - Internal Evaluation for X_std_ and X_mm_ datasets\n",
    "We can also perform an internal evaluation of each of the two dataset, with respect to the silhouette score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4babe2f4-f800-4f51-9c97-6059d4ca12cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def internal_evaluation(Y, cluster_assignment, suptitle:str=''):\n",
    "    cluster_label = np.sort(np.unique(cluster_assignment)).tolist()\n",
    "\n",
    "    sil_sample_series = silhouette_samples(Y, labels=cluster_assignment)\n",
    "    y_lower = 0\n",
    "\n",
    "    y_centers = []\n",
    "\n",
    "    res_df = pd.DataFrame(0.0, index=cluster_label, columns=['Avg. silhouette'])\n",
    "    res_df.index.name = 'Cluster'\n",
    "\n",
    "    cmap_ = cm.rainbow\n",
    "    norm = plt.Normalize(np.min(cluster_label), np.max(cluster_label))\n",
    "    handles = []\n",
    "\n",
    "    ax:plt.Axes\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10,6))\n",
    "\n",
    "    for i in cluster_label:\n",
    "        ith_cluster_silhouette_values = np.sort(sil_sample_series[cluster_assignment == i])\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        y_centers.append(y_lower + size_cluster_i/2.0)\n",
    "\n",
    "        color = cmap_(norm(i))\n",
    "        ax.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        avg_cluster_silhouette_score = np.mean(ith_cluster_silhouette_values) \n",
    "        res_df.loc[cluster_label[i], :] = avg_cluster_silhouette_score\n",
    "        y_lower = y_upper + 10\n",
    "\n",
    "        handles.append(Line2D([0], [0], marker='o', color='w', markerfacecolor=color, markersize=10, label=f'{centroids_names[i]}: {avg_cluster_silhouette_score:.3f}'))\n",
    "\n",
    "    fig.suptitle(f\"{suptitle}, Silhouette Plot for the Various Clusters\")\n",
    "\n",
    "    ax.set_xlabel(\"Silhouette Coefficient Values\")\n",
    "    ax.set_ylabel(\"Cluster Label\") \n",
    "    ax.set_yticks(y_centers, cluster_label)\n",
    "    \n",
    "    avg_silhouette_score = np.mean(sil_sample_series) \n",
    "    ax.axvline(x=avg_silhouette_score, color=\"red\", linestyle=\"--\") \n",
    "\n",
    "    ax.set_title(f\"Avg Silhouette Score: {avg_silhouette_score:.3f}\")\n",
    "\n",
    "    handles.reverse()\n",
    "    fig.legend(handles=handles, title='avg. sil. score per cluster', loc='center right')\n",
    "\n",
    "    return res_df\n",
    "\n",
    "res_std_df = internal_evaluation(Ystd_m_df, cluster_assignment_std, 'Standardized dataset'); plt.show()\n",
    "# display(res_std_df)\n",
    "\n",
    "res_mm_df = internal_evaluation(Ymm_m_df, cluster_assignment_mm, 'Normalized dataset'); plt.show()\n",
    "# display(res_mm_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a337e03a",
   "metadata": {},
   "source": [
    "The average silhouette score for the standardized dataset is 0.176, whilst for the normalized dataset, it is slightly higher at 0.179. Both scores suggest weak clustering performance as they are far below 0.5, implying some overlap between clusters or not optimal separation. <br>\n",
    "We have to recall that the silhouette score varies in the interval $[-1,1] \\in \\mathbb{R}$ with $1$ being the best score and $-1$ being the worst score.\n",
    "\n",
    "The slight increase in the score for the normalization preprocess implies a small improvement in clusters quality.\n",
    "\n",
    "Regarding standardization, this technique helped with the social realist cluster (2) as the silhouette score associated is higher and there aren't any points with a negative score. While normalization helped more with the other two clusters, as there are less negative valued scores and they are lower in magnitude.\n",
    "\n",
    "**But**, both methods show a wide range of values, so there are customers poorly assigned, or some clusters may overlap, resulting in a weaker separation.\n",
    "\n",
    "**In conclusion**, while both preprocessing methods resulted in similar outcomes, normalization showed a marginal improvement in clustering quality. However, the weak silhouette scores highlight that either the chosen number of clusters, PCA configuration, or feature engineering require refinement."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dd3d5f",
   "metadata": {},
   "source": [
    "## Final comments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98b056e",
   "metadata": {},
   "source": [
    "To conclude the analysis we are quite satisfied with the methodology followed, being able to extract meaningful insights from the data. <br>\n",
    "In contrast, the final results highlight a poor clustering results, which to be honest is mainly expected due to the fact that we are randomly sampling the data and that we are selecting only the first $5$ Principal Components (PCs) to perform clustering, thus resulting in a final cumulative variance score obtained of approximately far below the low threshold of $33\\%$.\n",
    "\n",
    "In spite of this, some profiles were found and most importantly, both dataframes, for the most part, were able to identify the same profiles.\n",
    "\n",
    "To improve the clustering performance I would for sure pick a higher number of Principal Components, moreover, it could also be useful or at the very least interesting to pick a different clustering algorithm, for example DBSCAN or Bisecting KMeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "668ff371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: check coherence of comments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
