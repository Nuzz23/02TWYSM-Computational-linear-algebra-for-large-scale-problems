{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>INTRODUCTION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This document represents a small introduction on how to do and what to look for when doing spectral clustering. <br>\n",
    "Starting from either one of this two files: <em>Spiral.csv</em> or <em>Circle.csv</em>.<br>\n",
    "We are going to implement spectral clustering: moreover, we will be building the similarity matrix $W$ the distance matrix $D$ and the Laplacian matrix $L$.<br>\n",
    "Then, we will evaluate eigenvectors and eigenvalues of $L$, compute $U$, and finally clusterize the starting points.\n",
    "<br><br>\n",
    "<b>The authors of the following files are:\n",
    "- Nunzio Licalzi, s344860\n",
    "- Romeo Vercellone, s341967</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>MODULES AND CONSTANTS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>MODULES AND LIBRARIES</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERAL IMPORTS\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from scipy.linalg import eigh\n",
    "\n",
    "\n",
    "# PLOTTING IMPORT\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "# DISTANCE FUNCTION\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "\n",
    "\n",
    "# CLUSTERING ALGORITHMS\n",
    "from sklearn.cluster import DBSCAN, BisectingKMeans, KMeans, SpectralClustering\n",
    "\n",
    "# METRICS\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# OUTPUT FUNCTION\n",
    "from IPython.display import display, Math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>CONSTANTS</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPIRAL_INPUT_FILE = 'Spiral.csv'\n",
    "CIRCLE_INPUT_FILE = 'Circle.csv'\n",
    "EPS = 1e-24\n",
    "NEIGHBORS = 10\n",
    "EIGENPAIRS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>0 - READ FUNCTIONS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>readSpiralInput</b>\n",
    "reads the input file containing the spiral data, \n",
    "<br> accepts as input the path to the csv file containing the data\n",
    "<br> returns a pandas dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readSpiralInput(path:str=SPIRAL_INPUT_FILE)->pd.DataFrame:\n",
    "    return pd.read_csv(filepath_or_buffer=path, header=None).set_axis(labels=['x', 'y', 'cluster'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>readCircleInput</b>\n",
    "reads the input file containing the circle data\n",
    "<br> accepts as input the path to the csv file containing the data\n",
    "<br> returns a pandas dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readCircleInput(path:str=CIRCLE_INPUT_FILE)->pd.DataFrame:\n",
    "    return pd.read_csv(filepath_or_buffer=path, header=None).set_axis(labels=['x', 'y'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print the data (since we are dealing with 2D points) to understand the spacial placement of the points and to discuss the most suited cluster algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotPoint(points:pd.DataFrame, title:str=None)->None:\n",
    "    plt.subplots(1,1,figsize=(8,8))\n",
    "    plt.scatter(points['x'], points['y'], c=points['cluster'] if 'cluster' in points.columns else 'blue')\n",
    "    plt.title(title or 'Points')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_SPIRAL = True\n",
    "data = readSpiralInput(SPIRAL_INPUT_FILE) if USE_SPIRAL else readCircleInput(CIRCLE_INPUT_FILE)\n",
    "plotPoint(data, title='points of ' + 'spiral.csv' if USE_SPIRAL else 'Circle.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>1 - BUILDING $W$ SIMILARITY MATRIX </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of data points X and the similarity function:\n",
    "$ s_{i, j} = exp\\left( - \\frac{||X_i - X_j||^2}{2\\sigma^2}\\right)$\n",
    "\n",
    "construct the <em>k-nearest neighborhood</em> similarity graph and its adjacency\n",
    "matrix $W$ using $\\sigma = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildSimilarityMatrix(circle:pd.DataFrame, k:int=NEIGHBORS, sigma:int=1):\n",
    "    matrix = rbf_kernel(circle, gamma=sigma/2)\n",
    "    np.fill_diagonal(matrix, 0)\n",
    "\n",
    "    mask = np.zeros(matrix.shape, dtype=bool)\n",
    "    mask[np.arange(matrix.shape[0])[:, None], np.argpartition(-matrix, k, axis=1)[:, :k]] = True\n",
    "    \n",
    "\n",
    "    matrix = np.where(mask, matrix, 0)\n",
    "    \n",
    "    for i in range(matrix.shape[0]):\n",
    "        for j in range(matrix[i, :].shape[0]):\n",
    "            if matrix[i, j] > EPS:\n",
    "                matrix[j, i] = matrix[i, j]\n",
    "    \n",
    "    return matrix\n",
    "\n",
    "W = buildSimilarityMatrix(data[['x', 'y']], NEIGHBORS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4><b>Symmetry problem</b></h4>\n",
    "The main problem that may occur when building this matrix is related to the required symmetry. \n",
    "\n",
    "Moreover, the given matrix $W$ must be symmetric since we are dealing with a weighted, non-oriented graph. <br>\n",
    "However, this may not always be the case because, even if weights are computed symmetrically (i.e., $\\forall i,j, s_{i,j} = s_{j,i}$), considering the $k-nearest \\text{ }neighbors$ may create asymmetry:  \n",
    "If a vertex $V$ is a neighbor of $V'$, we cannot be sure of the opposite, as shown here.<br>\n",
    "<img src='./pictures/output.png' alt='KNN problem'></img>\n",
    "<br>\n",
    "To address this issue, we can use one of two approaches (relaxing the constraint of having exactly $k$ neighbors):\n",
    "<ul>\n",
    "    <li>Impose that if <em>V</em> is a neighbor of <em>V'</em>, then <em>V'</em> is also a neighbor of <em>V</em>, thus potentially allowing more than <em>k</em> neighbors for a single vertex.</li>\n",
    "    <li>Impose that if <em>V</em> is not a neighbor of <em>V'</em>, then <em>V'</em> is also not a neighbor of <em>V</em>, thus potentially allowing fewer than <em>k</em> neighbors for a single vertex.</li>\n",
    "</ul>\n",
    "We have chosen the first approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>2 - BUILD DEGREE MATRIX $D$ AND LAPLACIAN MATRIX $L$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the degree matrix $D$ and the <b>Laplacian matrix</b> $L = D − W$.<br>\n",
    "Note: we can also build the <b>normalized symmetric Laplacian matrix</b>\n",
    "$L_{sym} \\in R^{N,N}$ that is defined as $ L_{sym} := D ^{− \\frac{1}{2}} LD^{−\\frac{1}{2}} = I − D^{-\\frac{1}{2}}WD^{-\\frac{1}{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "def buildDegreeMatrix(matrix:np)->np:\n",
    "    return np.diag(matrix.sum(axis=1))\n",
    "\n",
    "def buildLaplacianMatrix(D:np, W:np, computeNormalizedLaplacian: bool=False) -> np:\n",
    "    if not computeNormalizedLaplacian:\n",
    "        return D-W\n",
    "\n",
    "    D[np.diag_indices(n=D.shape[0])]**= -1/2 \n",
    "    return np.eye(N=D.shape[0]) - D @ W @ D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>setAsSparseMatrix</b>\n",
    "Given a matrix as input, returns the same matrix in a sparse format. <br>\n",
    "We have chosen to use an approach similar to the <b>modified CSR sparse format</b> to prioritize the diagonal of the matrix. <br>\n",
    "Which is particularly convenient when dealing with matrices such as the degree matrix $D$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setAsSparseMatrix(matrix:np, method:sparse.dia_array|None=sparse.dia_array)->sparse:\n",
    "    return method(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the Laplacian matrix is built using the previously declared function. <br>\n",
    "Moreover, since we will only be working with $L$ from now on, we can convert $D$ and $W$ into a sparse matrix format. <br>\n",
    "We prefer not to convert $L$ since it will still be used for many different purposes. <br>\n",
    "Due to the structure of $W$ we opted to convert it into a modified CSR format, whilst we only store the diagonal of D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = buildLaplacianMatrix(D:=buildDegreeMatrix(W), W, computeNormalizedLaplacian=True)\n",
    "D, W = D[np.diag_indices(D.shape[0])], setAsSparseMatrix(W, method=sparse.csr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>3 - CONNECTED COMPONENT OF THE GRAPHS </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the number of connected components of the similarity graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotLaplacian(L:np)->None:\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    plt.spy(L)\n",
    "    ax.set_title('Similarity graph')\n",
    "    plt.show()\n",
    "    \n",
    "plotLaplacian(L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First and foremost, as expected, the Laplacian matrix $L$ is symmetric. <br>\n",
    "Moreover, it is a sparse matrix, with most of the non-zero elements located near the main diagonal. <br>\n",
    "Finally, and most importantly, we can conclude that the graph is primarily composed of three main components (represented by the three blocks along the diagonal), which are weakly connected, as indicated by the small black patches far from the main diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>4/5 - EIGENVALUES AND EIGENVECTORS OF $L$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute some of the small eigenvalues of $L$ and use their values to choose a suitable number of clusters $M$ for the data points. <br>\n",
    "From what we have previously observed, we can conclude that the graph has $3$ main components. <br>\n",
    "However, we are going to compute the first $10$ eigenvalues, mainly because the graph also shows $6$ more patches with non-zero values (even though the patches are symmetric), indicating a weak connection between the $3$ main components of the graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>USING SCIPY IMPLEMENTATION</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily compute eigenvalues using the eigh function within scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printEigenValues(eigVal:np, legendText:str)->None:\n",
    "    n = eigVal.shape[0]\n",
    "    print(f\"Computed the following {n} eigenvalues\" )\n",
    "    for i, item in enumerate(eigVal):\n",
    "        display(Math(rf\"\\hspace{{2em}}\\lambda_{{{i}}} = {item}\"))\n",
    "\n",
    "    plt.subplots(1,1,figsize=(8,8))\n",
    "    plt.plot(values:=range(1, n+1), eigVal)\n",
    "    plt.scatter(values, eigVal, c='red', label=legendText)\n",
    "    plt.xticks(values)\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(f\"First {n} Eigenvalues\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(visible=True)\n",
    "    plt.show()\n",
    "    \n",
    "printEigenValues(eigh(L,subset_by_index=[0, 9],eigvals_only=True), \"eigenvalue's of scipy's method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <b>INVERSE POWER METHOD</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "To evaluate the eigenvalues we can also implement the <b>inverse power method</b> algorithm. \n",
    "<h4><b>Theoretical aspects</b></h4>\n",
    "\n",
    "Let $A \\in R^{n,n}$, we can define $\\lambda_n$ as the eigenvalue of A with smallest magnitude (i.e. $|\\lambda_n| \\leq |\\lambda_i|, \\forall i \\in [1,n]$). <br>\n",
    "Moreover, assuming A is invertible (and this is our case) we have that the eigenvalues of the inverse of A are the inverse of the eigenvalues of the matrix A <br>(i.e. $\\forall i \\in [1, n], Ax_i = \\lambda_i x_i \\implies A^{-1}x_i = \\frac{1}{\\lambda_i}x_i$ if A is invertible ).<br>\n",
    "Thus implying that if $|\\lambda_n| \\leq |\\lambda_i|, \\forall i \\in [1,n]$ then $\\frac{1}{|\\lambda_n|} \\geq \\frac{1}{|\\lambda_i|}, \\forall i \\in [1,n]$ <br>\n",
    "Finally, we can compute the eigenvalues by using the power method applied to the inverse of A, i.e. $A^{-1}$.\n",
    "<br>\n",
    "<h4><b>Algorithm</b></h4>\n",
    "This is the algorithm that we are going to implement:\n",
    "<ul>\n",
    "<li><b>Inputs</b>: \n",
    "\n",
    "$A$ (matrix), $V$ (initial guess vector), $p$ (initial guess for eigenvalue), $maxIter$ (maximum iterations), $tol$ (tolerance)\n",
    "\n",
    "</li>\n",
    "<li><b>Algorithm</b>:</li>\n",
    "<ol>\n",
    "    <li>\n",
    "\n",
    "Normalize the initial guess vector $ V_0 \\leftarrow \\frac{V_0}{\\|V_0\\|_2} $\n",
    "</li>\n",
    "<li>\n",
    "\n",
    "Set $ \\mu_0 = 0 $ and initialize $ k = 0 $.</li>\n",
    "    <li>Repeat until convergence (i.e. $|\\mu_k -\\mu_{k-1}| \\leq tol\\cdot|\\mu_k| $) or $ k \\geq \\text{maxIter} $:\n",
    "        <ol type='I'>\n",
    "            <li>Compute $ V_{k+1} $ by solving $ (A - pI)V_k = V_{k+1} $.</li>\n",
    "            <li>Update $ \\mu_{k-1} = \\mu_k$ and $\\mu_k = V_{k+1} * V_{k} $.</li>\n",
    "            <li>Normalize $ V_{k+1} \\leftarrow \\frac{V_{k+1}}{\\|V_{k+1}\\|_2} $.</li>\n",
    "            <li>Update $k \\leftarrow k+1$</li>\n",
    "        </ol>\n",
    "    </li>\n",
    "</ol>\n",
    "<li><b>Outputs</b>: \n",
    "\n",
    "The eigenvalue $ \\lambda_j = p + \\frac{1}{\\mu_k} $ and the eigenvector $ V_k $.\n",
    "</li>\n",
    "</ul>\n",
    "\n",
    "The main issue is the fact that if we apply the inverse power method to the $L$ matrix $10$ times we are going to get the same eigenpair $10$ times.<br>\n",
    "Once computed the eigenpair, we have to move it out of the way so that we may compute the remaining eigenpairs.<br>\n",
    "To do so, we can use either one of two techniques: <b>Shifting</b> or <b>Deflation</b>.\n",
    "\n",
    "<h4><b>Shifting</b></h4>\n",
    "\n",
    "This technique can only be applied to symmetric matrix (and, luckily for us, $L$ is symmetric), this requirement is imposed so that we may deal with real eigenvalues and orthogonal eigenvectors <br>\n",
    "i.e. let $A \\in R^{n,n} \\text{ symmetric } \\implies \\forall i \\in [1, n], \\lambda_i \\in \\mathbb{R}$ and $\\forall j \\in [1, n], x_i * x_j = \\delta_{ij}$<br>\n",
    "We assume knowing the eigenpair $\\lambda_1, x_1$, given that eigenpair we can compute $A' = A - \\lambda_1\\cdot x_1 x_1^T \\in R^{n,n}$ with $x_1x_1^T \\in R^{n,n}$ and of rank 1. <br>\n",
    "The new matrix $A'$ has the same eigenvectors of $A$ and almost the same eigenvalues, except for the first one. <br>\n",
    "i.e. defining with $\\lambda_i$ the eigenvalues of $A$ and $\\lambda_i'$ the eigenvalues of $A'$ then $\\forall i \\in [2, n], \\lambda_i = \\lambda_i'$ and for $i=1 \\lambda_i' = 0$<br>\n",
    "Which implies that we have effectively created a new matrix that has the vector $\\lambda_1x_1$ into it's right kernel, thus removing it from it's eigenvalues and adding $0$ as eigenvalue.<br>\n",
    "\n",
    "When dealing with the <b>power method</b> this is sufficient, since we are always evaluating the eigenvalue with the highest magnitude (and $0$ is the lowest possible magnitude).<br>\n",
    "However when dealing with the <b>inverse power method</b> this may prove counter productive since $\\frac{1}{0+\\epsilon}$ (with $\\epsilon$ due to machine precision) is going to skyrocket, thus getting in the way when calculating eigenvalues with this method. <br>\n",
    "Instead we have to create $A'$ in such a way that the eigenvalue $\\lambda_1$ gets a really high magnitude, such that, $\\frac{1}{\\lambda_1}$ becomes so low that is no longer a problem with this method. <br>\n",
    "The best way to do so, in our case, is the following $A' = A + \\alpha\\lambda_1\\cdot x_1 x_1^T$ with $ \\alpha \\approx 10^{15}$ \n",
    "\n",
    "<h4><b>Deflation</b></h4>\n",
    "\n",
    "Let $A \\in R^{n,n}$ and let $(\\lambda_1, x_1)$ be an eigenpair of $A$ and let $P_1 \\in R^{n,n}$ orthogonal such that: <br>\n",
    "$B_1 = P_1 A P_1^T = \\begin{bmatrix} \\lambda_1 --- b_1 --- \\\\ 0 \\hspace{7em} \\\\ | \\hspace{1em} A_2  \\in R^{n-1, n-1}\\\\ 0 \\hspace{7em} \\end{bmatrix} \\in R^{n,n}$.<br>\n",
    "The eigenvalues of $A_2$ will almost be the same of the ones of $A$, in fact, defining as $\\lambda$ the eigenvalues of $A$ and as $\\lambda'$ the eigenvalues of $A_2$, then, $\\forall i \\in [2, n], \\lambda_i = \\lambda_{i-1}' \\implies \\{\\lambda'\\} \\cup \\{\\lambda_1\\} = \\{\\lambda\\}$. <br>\n",
    "We have to notice that the first column of $B_1 = B_1*e_1^n = \\lambda_1 \\cdot e_1^n $. <br>\n",
    "We can define $P_1 = I_n - 2 \\frac{(x_1+e^n_1)(x_1+e^n_1)^T}{|| x_1 + e^n_1||_2}$. <br>\n",
    "Moreover, we can see how once we have computed the eigenpair $(\\lambda_2, x_2)$ we can use again this procedure to deflate $A_2$ being careful with using vector of size $n-1$ (i.e. $e^{n-1}_1$). <br>\n",
    "Moreover, with this method, the computed eigenvalues won't vary, but eigenvectors will! <br>\n",
    "To get the eigenvectors $x_i$ of $A$ we have to compute $x_i = P_1 P_2 P_3 ...P_{k-1}x_i'$.\n",
    "\n",
    "<br><br><br>\n",
    "Now we can apply the <b>Inverse power method with either deflation or shifting</b> (both solution are proposed) to $L$ to compute the first ten eigenvalues and the corresponding eigenvectors.<br>\n",
    "The shape of $L$ is already pretty good, we don't have to modify it to make the algorithm more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>Inverse power method</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inversePowerMethodAlgorithm(A:np, V:np, p:float, maxIter:int=200, tol: float=EPS) -> list[float, np]:\n",
    "    V = V/np.linalg.norm(V,2)\n",
    "    pI = p*np.eye(A.shape[0])         # Precomputed to have a faster execution\n",
    "    mu_prev, mu, k = 0, 1, 0\n",
    "        \n",
    "    while k < maxIter and np.abs(mu - mu_prev)>=tol*np.abs(mu):\n",
    "        Vk = np.linalg.solve(A-pI, V)\n",
    "        mu, mu_prev = np.dot(Vk,V), mu\n",
    "        V = Vk/np.linalg.norm(Vk, 2)\n",
    "        k += 1\n",
    "        \n",
    "    return p + 1/mu, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Using Shifting </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyInversePowerMethodWithShifting(A:np, numEig:int=EIGENPAIRS, maxIter:int=1e4, tol: float=EPS)->list[list[float], list[np]]:\n",
    "    temp = A.copy()\n",
    "    \n",
    "    # Pre allocated for speed \n",
    "    eigVal = np.zeros(numEig)\n",
    "    eigVecs = np.zeros((A.shape[0], numEig))\n",
    "\n",
    "    i = 0\n",
    "    while i < numEig:\n",
    "        eigVal[i], eigVecs[:, i] = inversePowerMethodAlgorithm(temp, np.random.rand(A.shape[0]), p=1e-16, maxIter=maxIter, tol=tol)\n",
    "\n",
    "        vect = eigVecs[:, i].reshape(-1, 1)\n",
    "        temp += 1e8*eigVal[i] * (vect @ vect.T)  \n",
    "        \n",
    "        if np.abs(eigVal[i]) < 1e-14 or np.abs(eigVal[i]) >1e-6:\n",
    "            i+=1 \n",
    "        \n",
    "    return eigVal, eigVecs    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Using Deflation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyInversePowerMethodWithDeflation(A:np, numEig:int=EIGENPAIRS, maxIter:int=1e4, tol: float=EPS)->list[list[float], list[np]]:\n",
    "    matrix = A.copy()\n",
    "    \n",
    "    # Pre allocated for speed \n",
    "    eigVal = np.zeros(numEig)\n",
    "    eigVecs = np.zeros((A.shape[0], numEig))\n",
    "\n",
    "    # creates the tensor for storing P\n",
    "    P = np.array([np.eye(matrix.shape[0])]*numEig)   # [#tensor, row, col]\n",
    "\n",
    "    for i in range(numEig):        \n",
    "        eigVal[i], eigVecs[i:, i] = inversePowerMethodAlgorithm((matrix:= P[i-1]@matrix@P[i-1].T)[i:, i:], \n",
    "                                                np.random.rand(A.shape[0]-i), p=1e-5, maxIter=maxIter, tol=tol)\n",
    "    \n",
    "        P[i, i:, i:]=P[i, i:, i:] - 2*(temp:=(eigVecs[i:, i].reshape(-1,1) + np.eye(N=matrix.shape[0]-i, M=1))).T*temp/np.linalg.norm(temp, 2)**2\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # raise ValueError('da implementare autovettori')\n",
    "        \n",
    "    return eigVal, eigVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>VISUALIZING AND COMPARING RESULTS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can compare the results between the two different implementations, taking numpy ones as a reference:\n",
    "<ul>\n",
    "<li><b>Shifting</b>: implementation of the <b>Inverse power method</b> using shifting.</li>\n",
    "<li><b>Deflation</b>: implementation of the <b>Inverse power method</b> using deflation.</li>\n",
    "<li><b>Scipy</b>: calculating the eigenpairs via the eigh function within the scipy module.</li>\n",
    "</ul>\n",
    "<b>NOTE:</b> we will only compare eigenvalues and not eigenvectors due to the high dimensionality of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparisonBetweenEigenvaluesMethod(df:pd.DataFrame, plotSingle:bool=False)->None:\n",
    "    print(f\"Eigenvalues calculated : {df.shape[0]}\")\n",
    "    display(df)\n",
    "    param = {   'shifting':{'c':'red', 'marker':'*'}, \n",
    "                'deflation':{'c':'blue', 'marker':'x'}, \n",
    "                'scipy':{'c':'green', 'marker':'^'}}\n",
    "\n",
    "    fig, ax = plt.subplots(1,1,figsize=(16,8))    \n",
    "    for col in df.columns[::-1]:\n",
    "        ax.plot(df.index, df[col])\n",
    "        ax.scatter(df.index, df[col], label=col, **param[col])\n",
    "        \n",
    "\n",
    "    plt.xticks(range(df.index.shape[0]), rotation=-60)\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(\"Direct comparison between eigenvalues methods\")\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(visible=True)\n",
    "    plt.show()\n",
    "    if plotSingle:\n",
    "        fig, ax = plt.subplots(3,1,figsize=(8, 24))\n",
    "        for i,col in enumerate(df):\n",
    "            ax[i].plot(df.index, df[col])\n",
    "            ax[i].scatter(df.index, df[col], label=col, **param[col])\n",
    "            \n",
    "            ax[i].set_xticks(range(1, df.index.shape[0]+1))\n",
    "            ax[i].set_xticklabels(df.index, rotation=-80)\n",
    "            ax[i].set_ylabel(\"Eigenvalue\")\n",
    "            ax[i].set_title(f\"First {df.shape[0]} Eigenvalues with method {col}\")\n",
    "            ax[i].legend(loc='upper left')\n",
    "            ax[i].grid(visible=True)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "comparisonBetweenEigenvaluesMethod(pd.DataFrame(\n",
    "                                    zip(applyInversePowerMethodWithShifting(L)[0], \n",
    "                                        applyInversePowerMethodWithDeflation(L)[0], \n",
    "                                        eigh(L,subset_by_index=[0, 10],eigvals_only=True)),\n",
    "    columns=['shifting', 'deflation', 'scipy'], index=[f'\\u03BB{i+1}' for i in range(EIGENPAIRS)], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can compute the eigenpairs of $L$ with a method of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeEigenpairs(L:np, numEig:int=EIGENPAIRS, method:str|None='scipy', maxIter:int|None=1e4, tol:float=EPS)->list[np, np]:\n",
    "    match method.strip().upper() if method else method:\n",
    "        case 'DEFLATION': return applyInversePowerMethodWithDeflation(A=L, numEig=numEig, maxIter=maxIter, tol=tol)\n",
    "        case 'SHIFTING': return applyInversePowerMethodWithShifting(A=L, numEig=numEig, maxIter=maxIter, tol=tol)\n",
    "        case _:  return (eigenpairs :=eigh(L,subset_by_index=[0, numEig-1],eigvals_only=False))[0], eigenpairs[1]\n",
    "    \n",
    "eigenvalues, U = computeEigenpairs(L, method='*')\n",
    "\n",
    "# NOTE: from now on we won't be needing L so we can convert it to a sparse format to save memory space\n",
    "# L = setAsSparseMatrix(L, method=sparse.csr_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing solely the eigenvalues "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printEigenValues(eigenvalues, legendText='eigenvalues calculated')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(pd.DataFrame(U, index=[f'x[j, {i}]' for i in range(1, U.shape[0]+1)], columns=[f'x{i+1}' for i in range(U.shape[1])], dtype=np.float32)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Independently from the method chosen, as expected, the first $3$ eigenvalues are $0$ or are really close to $0$. <br>\n",
    "Then there is another group of $3$ eigenvalues with values closer to $0.005$ which indicates a fairly weak connection between the three main components. <br>\n",
    "Moreover there is, also, another group of $3$ eigenvalues, with values higher than the previous one.<br>\n",
    "Finally, we can see how, starting from the $10^{th}$ eigenvalue, they become bigger and bigger, thus indicating an even stronger connection. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>6 - CLUSTERING $U$</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For $i = 1, ..., N$ let $y_i \\in R^M$ be the vector corresponding to the $i-th$ row of $U$.\n",
    "<br> Cluster the points $i = 1, ..., N \\text{  } y_i\\in R^M$ with the <em>k-means algorithm</em> into clusters $C_1, ..., C_M$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClusteringAndCentroidsOfU(U: np.ndarray, centroids: np.ndarray, clusterLabels: np.ndarray) -> None:\n",
    "    \n",
    "    if U.shape[1] == 3:  \n",
    "        ax = plt.figure(figsize=(10, 8)).add_subplot(111, projection='3d')\n",
    "        \n",
    "        for label, color in zip(np.unique(clusterLabels), plt.cm.tab10(np.linspace(0, 1, len(np.unique(clusterLabels))))):\n",
    "            ax.scatter((points:=U[clusterLabels == label])[:, 0], points[:, 1], points[:, 2], c=[color], s=50, label=f'Cluster C{label}')\n",
    "\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], c='black', marker='*', s=200, label='Centroids')\n",
    "        ax.set_zlabel('Z')  \n",
    "    else:\n",
    "        fig, ax = plt.subplots(1,1, figsize=(8,8))\n",
    "        for label, color in zip(np.unique(clusterLabels), plt.cm.tab10(np.linspace(0, 1, len(np.unique(clusterLabels))))):\n",
    "            ax.scatter((points:=U[clusterLabels == label])[:, 0], points[:, 1], c=[color], s=50, label=f'Cluster C{label}')\n",
    "\n",
    "        ax.scatter(centroids[:, 0], centroids[:, 1], c='black', marker='*', s=200, label='Centroids')\n",
    "\n",
    "    # Titles and labels\n",
    "    ax.set_title('3D Scatter Plot with Clustering and Centroids')\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# Example Data and Usage\n",
    "temp = 3\n",
    "\n",
    "km = KMeans(n_clusters=temp, tol=EPS, random_state=23)\n",
    "clusterLabels = km.fit_predict(U[:, :temp])\n",
    "\n",
    "plotClusteringAndCentroidsOfU(U[:, :temp], centroids, clusterLabels) if 2<= (centroids:=km.cluster_centers_).shape[1]<=3 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'among Us ' + chr(sum(range(ord(min(str(not()))))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>7- ASSIGNING CLUSTER TO THE POINTS</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the original points in $X$ to the same clusters as their corresponding\n",
    "rows in $U$ and construct the clusters $A_1, ..., A_M$ with $A_i = \\{x_j : y_j \\in C_i\\}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['k-means']=clusterLabels\n",
    "# U can be transformed into a sparse matrix, we opted for a csr format, instead of the usual diagonal format\n",
    "# since the diagonal of U has no particular meaning \n",
    "# U = setAsSparseMatrix(U, method=sparse.csr_matrix)  \n",
    "# NOOOO, U IS NOT SPARSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b> 8 - PLOTTING CLUSTERS </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the clusters of points $X$ with different colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClusteringResults(df: pd.DataFrame) -> None:\n",
    "    columns: np.ndarray = df.columns.difference(['x', 'y']).sort_values()\n",
    "\n",
    "    # Create subplots for each clustering method\n",
    "    fig, ax = plt.subplots(columns.shape[0], 1, figsize=(8, 8 * columns.shape[0]))\n",
    "    ax = ax if len(columns) >1 else [ax]\n",
    "\n",
    "    for i, column in enumerate(columns):\n",
    "        scalarMap = cm.ScalarMappable(norm=colors.Normalize(vmin=min(unique_labels:=np.unique(df[column])), vmax=max(unique_labels)), \n",
    "                                        cmap=plt.get_cmap('tab10', len(unique_labels)))\n",
    "\n",
    "        ax[i].scatter(df['x'],df['y'], c=[scalarMap.to_rgba(label) for label in df[column]],s=50)\n",
    "\n",
    "        for label in unique_labels:\n",
    "            ax[i].scatter([], [], color=scalarMap.to_rgba(label), label=f'Cluster {label}')\n",
    "\n",
    "        ax[i].set_title(f'Clustering Method: {column}')\n",
    "        ax[i].set_xlabel('X'),ax[i].set_ylabel('Y')\n",
    "        ax[i].legend(loc='upper left')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plotClusteringResults(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <b>9 - CLUSTERING METHODS COMPARISON</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the clusters for the same set of points with other clustering methods and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotClusters(df: pd.DataFrame, labels:np, title:str|None=None) -> None:\n",
    "    # Get unique cluster labels\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    \n",
    "    # Plot each cluster with a different color\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for label in unique_labels:\n",
    "        # Filter points belonging to the current cluster\n",
    "        cluster_points = df[labels == label]\n",
    "        \n",
    "        \n",
    "        # Plot the cluster points\n",
    "        plt.scatter(\n",
    "            cluster_points['x'],\n",
    "            cluster_points['y'],\n",
    "            label=f'Cluster {label}' if label != -1 else 'Noise',\n",
    "            s=50\n",
    "        )\n",
    "    \n",
    "    # Add legend and axis labels\n",
    "    plt.title(f'Clustered Points {title or \" \"}')\n",
    "    plt.xlabel('X'), plt.ylabel('Y')\n",
    "    plt.legend(loc='upper right', title='Clusters')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "# for spiral eps=1.2, min_samples=3\n",
    "# for circle eps=0.8 min_samples=3\n",
    "plotClusters(data, DBSCAN(eps=1.2 if USE_SPIRAL else 0.80, min_samples=2).fit_predict(data[['x','y']]), 'DBSCAN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotClusters(data, BisectingKMeans(n_clusters=3).fit_predict(data[['x','y']]), title='Bisecting KMeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nearest_neighbors\n",
    "plotClusters(data, \n",
    "             SpectralClustering(n_clusters=3, random_state=23, n_init=20, gamma=0.5, \n",
    "                                affinity='rbf', n_neighbors=10, eigen_tol=EPS, n_jobs=-1).fit_predict(data[['x','y']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "implementare un garifco come time series dove sull'asse x metti i vari n clusters, sul y i silohuette score e le varie linee sono i metodi diversi di clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
